{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To: Land-Use-Land-Cover Prediction for Slovenia\n",
    "\n",
    "This notebook shows the steps towards constructing a machine learning pipeline for predicting the land use and land cover for the region of Republic of Slovenia. We will use satellite images obtained by ESA's Sentinel-2 to train a model and use it for prediction. The example will lead you through the whole process of creating the pipeline, with details provided at each step.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "### Requirements\n",
    "\n",
    "In order to run the example you'll need a Sentinel Hub account. If you do not have one yet, you can create a free trial account at [Sentinel Hub webpage](https://services.sentinel-hub.com/oauth/subscription). If you are a researcher you can even apply for a free non-commercial account at [ESA OSEO page](https://earth.esa.int/aos/OSEO).\n",
    "\n",
    "Once you have the account set up, login to [Sentinel Hub Configurator](https://apps.sentinel-hub.com/configurator/). By default you will already have the default configuration with an **instance ID** (alpha-numeric code of length 36). For this tutorial we recommend that you create a new configuration (`\"Add new configuration\"`) and set the configuration to be based on **Python scripts template**. Such configuration will already contain all layers used in these examples. Otherwise you will have to define the layers for your configuration yourself.\n",
    "\n",
    "After you have prepared a configuration please put configuration's **instance ID** into `sentinelhub` package's configuration file following the [configuration instructions](http://sentinelhub-py.readthedocs.io/en/latest/configure.html).\n",
    "\n",
    "### Overview\n",
    "\n",
    "#### Part 1:\n",
    "\n",
    "1. Define the Area-of-Interest (AOI):\n",
    "   * Obtain the outline of Slovenia (provided)\n",
    "   * Split into manageable smaller tiles\n",
    "   * Select a small 3x3 area for classification\n",
    "2. Use the integrated [sentinelhub-py](https://github.com/sentinel-hub/sentinelhub-py) package in order to fill the EOPatches with some content (band data, cloud masks, ...)\n",
    "   * Define the time interval (this example uses the whole year of 2017)\n",
    "3. Add additional information from band combinations (norm. vegetation index - NDVI, norm. water index - NDWI)\n",
    "4. Add a reference map (provided)\n",
    "   * Convert provided vector data to raster and add it to EOPatches\n",
    "   \n",
    "#### Part 2:\n",
    "\n",
    "5. Prepare the training data\n",
    "   * Remove too cloudy scenes\n",
    "   * Perform temporal interpolation (filling gaps and resampling to the same dates)\n",
    "   * Apply erosion \n",
    "   * Random spatial sampling of the EOPatches\n",
    "   * Split patches for training/validation\n",
    "6. Construct and train the ML model\n",
    "   * Make the prediction for each patch \n",
    "7. Validate the model\n",
    "8. Visualise the results\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Firstly, some necessary imports\n",
    "\n",
    "# Jupyter notebook related\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Built-in modules\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "from enum import Enum\n",
    "\n",
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Machine learning \n",
    "import lightgbm as lgb\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Imports from eo-learn and sentinelhub-py\n",
    "from eolearn.core import EOTask, EOPatch, LinearWorkflow, FeatureType, OverwritePermission, \\\n",
    "    LoadFromDisk, SaveToDisk, EOExecutor\n",
    "from eolearn.io import S2L1CWCSInput, ExportToTiff\n",
    "from eolearn.mask import AddCloudMaskTask, get_s2_pixel_cloud_detector, AddValidDataMaskTask\n",
    "from eolearn.geometry import VectorToRaster, PointSamplingTask, ErosionTask\n",
    "from eolearn.features import LinearInterpolation, SimpleFilterTask\n",
    "from sentinelhub import BBoxSplitter, BBox, CRS, CustomUrlParam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "## 1. Define the Area-of-Interest (AOI):\n",
    "\n",
    "* A geographical shape of Slovenia was taken from [Natural Earth database](http://www.naturalearthdata.com/downloads/10m-cultural-vectors/) and a buffer was applied. The shape is available in repository: `example_data/svn_buffered.geojson`\n",
    "* Convert it to selected CRS: taken to be the CRS of central UTM tile (UTM_33N)\n",
    "* Split it into smaller, manageable, non-overlapping rectangular tiles\n",
    "* Select a small 3x3 area for classification\n",
    "\n",
    "Be sure that your choice of CRS is the same as the CRS of your reference data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get country boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where data for running the notebook is located\n",
    "DATA_FOLDER = os.path.join('.', '..', 'data')\n",
    "\n",
    "# Folder where data will be stored\n",
    "OUTPUT_FOLDER = os.path.join('.', '..', 'outputs')\n",
    "if not os.path.isdir(OUTPUT_FOLDER):\n",
    "    os.mkdir(OUTPUT_FOLDER)\n",
    "\n",
    "# Load geojson file\n",
    "country = gpd.read_file(os.path.join(DATA_FOLDER, 'svn_buffered.geojson'))\n",
    "\n",
    "# Convert CRS to UTM_33N\n",
    "country_crs = CRS.UTM_33N\n",
    "country = country.to_crs(crs={'init': CRS.ogc_string(country_crs)})\n",
    "\n",
    "# Get the country's shape in polygon format\n",
    "country_shape = country.geometry.values.tolist()[-1]\n",
    "\n",
    "# Plot country\n",
    "country.plot()\n",
    "plt.axis('off');\n",
    "\n",
    "# Print size \n",
    "print('Dimension of the area is {0:.0f} x {1:.0f} m2'.format(country_shape.bounds[2] - country_shape.bounds[0],\n",
    "                                                             country_shape.bounds[3] - country_shape.bounds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to smaller tiles and choose a 3x3 area\n",
    "\n",
    "A 3x3 EOPatch sample, where each EOPatch has around 330 x 330 pixels at 10 meter resolution (~300 MB per EOPatch), is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the splitter to obtain a list of bboxes\n",
    "bbox_splitter = BBoxSplitter([country_shape], country_crs, (25 * 3, 17 * 3))\n",
    "\n",
    "bbox_list = np.array(bbox_splitter.get_bbox_list())\n",
    "info_list = np.array(bbox_splitter.get_info_list())\n",
    "\n",
    "# For the future examples, we will be using a specific set of patches,\n",
    "# but you are free to change the patch ID numbers in the scope of this example\n",
    "# Select a central patch\n",
    "ID = 1549 \n",
    "\n",
    "# Obtain surrounding patches\n",
    "patchIDs = []\n",
    "for idx, [bbox, info] in enumerate(zip(bbox_list, info_list)):\n",
    "    if (abs(info['index_x'] - info_list[ID]['index_x']) <= 1 and\n",
    "        abs(info['index_y'] - info_list[ID]['index_y']) <= 1):\n",
    "        patchIDs.append(idx)\n",
    "    \n",
    "# Change the order of the patches (used for plotting later)\n",
    "patchIDs = np.transpose(np.fliplr(np.array(patchIDs).reshape(3, 3))).ravel()\n",
    "    \n",
    "# Prepare info of selected EOPatches\n",
    "geometry = [Polygon(bbox.get_polygon()) for bbox in bbox_list[patchIDs]]\n",
    "idxs_x = [info['index_x'] for info in info_list[patchIDs]]\n",
    "idxs_y = [info['index_y'] for info in info_list[patchIDs]]\n",
    "\n",
    "gdf = gpd.GeoDataFrame({'index_x': idxs_x, 'index_y': idxs_y}, \n",
    "                       crs={'init': CRS.ogc_string(country_crs)}, \n",
    "                       geometry=geometry)\n",
    "\n",
    "# save to shapefile\n",
    "shapefile_name = os.path.join(OUTPUT_FOLDER, 'selected_3x3_bboxes_slovenia_small.shp')\n",
    "gdf.to_file(shapefile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = gdf['geometry'][0]\n",
    "x1, y1, x2, y2 = poly.bounds\n",
    "aspect_ratio = (y1 - y2) / (x1 - x2)\n",
    "\n",
    "# content of the geopandas dataframe\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontdict = {'family': 'monospace', 'weight': 'normal', 'size': 11}\n",
    "\n",
    "# if bboxes have all same size, estimate offset\n",
    "xl, yl, xu, yu = gdf.geometry[0].bounds\n",
    "xoff, yoff = (xu - xl) / 3, (yu - yl) / 5\n",
    "\n",
    "# figure\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "gdf.plot(ax=ax,facecolor='w',edgecolor='r',alpha=0.5)\n",
    "country.plot(ax=ax, facecolor='w',edgecolor='b',alpha=0.5)\n",
    "ax.set_title('Selected 3x3  tiles from Slovenia (25 x 17 grid)');\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. - 4. Fill EOPatches with data:\n",
    "\n",
    "Now it's time to create EOPatches and fill them with Sentinel-2 data using Sentinel Hub services. We will add the following data to each EOPatch:\n",
    "\n",
    "* L1C custom list of bands [B02, B03, B04, B08, B11, B12], which corresponds to [B, G, R, NIR, SWIR1, SWIR2] wavelengths.\n",
    "\n",
    "* SentinelHub's cloud probability map and cloud mask\n",
    "\n",
    "Additionally, we will add:\n",
    "\n",
    "* Calculated NDVI, NDWI, euclidean NORM information\n",
    "\n",
    "* A mask of validity, based on acquired data from Sentinel and cloud coverage. Valid pixel is if:\n",
    "   \n",
    "    1. IS_DATA == True\n",
    "    2. CLOUD_MASK == 0 (1 indicates that pixel was identified to be covered with cloud)\n",
    "\n",
    "An EOPatch is created and manipulated using EOTasks, which are chained in an EOWorkflow. In this example the final workflow is executed on all patches, which are saved to the specified directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some needed custom EOTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentinelHubValidData:\n",
    "    \"\"\"\n",
    "    Combine Sen2Cor's classification map with `IS_DATA` to define a `VALID_DATA_SH` mask\n",
    "    The SentinelHub's cloud mask is asumed to be found in eopatch.mask['CLM']\n",
    "    \"\"\"\n",
    "    def __call__(self, eopatch):        \n",
    "        return np.logical_and(eopatch.mask['IS_DATA'].astype(np.bool), \n",
    "                              np.logical_not(eopatch.mask['CLM'].astype(np.bool)))\n",
    "    \n",
    "class CountValid(EOTask):   \n",
    "    \"\"\"\n",
    "    The task counts number of valid observations in time-series and stores the results in the timeless mask.\n",
    "    \"\"\"\n",
    "    def __init__(self, count_what, feature_name):\n",
    "        self.what = count_what\n",
    "        self.name = feature_name\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        eopatch.add_feature(FeatureType.MASK_TIMELESS, self.name, np.count_nonzero(eopatch.mask[self.what],axis=0))\n",
    "        \n",
    "        return eopatch\n",
    "\n",
    "\n",
    "class NormalizedDifferenceIndex(EOTask):   \n",
    "    \"\"\"\n",
    "    The tasks calculates user defined Normalised Difference Index (NDI) between two bands A and B as:\n",
    "    NDI = (A-B)/(A+B).\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_name, band_a, band_b):\n",
    "        self.feature_name = feature_name\n",
    "        self.band_a_fetaure_name = band_a.split('/')[0]\n",
    "        self.band_b_fetaure_name = band_b.split('/')[0]\n",
    "        self.band_a_fetaure_idx = int(band_a.split('/')[-1])\n",
    "        self.band_b_fetaure_idx = int(band_b.split('/')[-1])\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        band_a = eopatch.data[self.band_a_fetaure_name][..., self.band_a_fetaure_idx]\n",
    "        band_b = eopatch.data[self.band_b_fetaure_name][..., self.band_b_fetaure_idx]\n",
    "        \n",
    "        ndi = (band_a - band_b) / (band_a  + band_b)\n",
    "        \n",
    "        eopatch.add_feature(FeatureType.DATA, self.feature_name, ndi[..., np.newaxis])\n",
    "        \n",
    "        return eopatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE CELL 1\n",
    "# SOLUTIONS AT THE BOTTOM\n",
    "\n",
    "class EuclideanNorm(EOTask):   \n",
    "    \"\"\"\n",
    "    The tasks calculates Euclidian Norm of all bands within an array:\n",
    "    norm = sqrt(sum_i Bi**2),\n",
    "    where Bi are the individual bands within user-specified feature array.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_name, in_feature_name):\n",
    "        self.feature_name = feature_name\n",
    "        self.in_feature_name = in_feature_name\n",
    "    \n",
    "    def execute(self, eopatch):\n",
    "        arr = eopatch.data[self.in_feature_name]\n",
    "        norm = np.sqrt(np.sum(arr**2, axis=-1))\n",
    "        \n",
    "        # add new feature to eopatch        \n",
    "        eopatch.add_feature(FeatureType.DATA, self.feature_name, norm[..., np.newaxis])\n",
    "        return eopatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the workflow tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK FOR BAND DATA\n",
    "# add a request for B(B02), G(B03), R(B04), NIR (B08), SWIR1(B11), SWIR2(B12) \n",
    "# from default layer 'ALL_BANDS' at 10m resolution\n",
    "# Here we also do a simple filter of cloudy scenes. A detailed cloud cover \n",
    "# detection is performed in the next step\n",
    "custom_script = 'return [B02, B03, B04, B08, B11, B12];'\n",
    "add_data = S2L1CWCSInput(\n",
    "    layer='BANDS-S2-L1C', \n",
    "    feature=(FeatureType.DATA, 'BANDS'), # save under name 'BANDS'\n",
    "    custom_url_params={CustomUrlParam.EVALSCRIPT: custom_script}, # custom url for 6 specific bands\n",
    "    resx='10m', # resolution x\n",
    "    resy='10m', # resolution y\n",
    "    maxcc=0.8, # maximum allowed cloud cover of original ESA tiles\n",
    ")\n",
    "\n",
    "# TASK FOR CLOUD INFO\n",
    "# cloud detection is performed at 80m resolution \n",
    "# and the resulting cloud probability map and mask \n",
    "# are scaled to EOPatch's resolution\n",
    "cloud_classifier = get_s2_pixel_cloud_detector(average_over=2, dilation_size=1, all_bands=False)\n",
    "add_clm = AddCloudMaskTask(cloud_classifier, 'BANDS-S2CLOUDLESS', cm_size_y='80m', cm_size_x='80m', \n",
    "                           cmask_feature='CLM', # cloud mask name\n",
    "                           cprobs_feature='CLP' # cloud prob. map name\n",
    "                          )\n",
    "\n",
    "# TASKS FOR CALCULATING NEW FEATURES\n",
    "# NDVI: (B08 - B04)/(B08 + B04)\n",
    "# NDWI: (B03 - B08)/(B03 + B08)\n",
    "# NORM: sqrt(B02^2 + B03^2 + B04^2 + B08^2 + B11^2 + B12^2)\n",
    "ndvi = NormalizedDifferenceIndex('NDVI', 'BANDS/3', 'BANDS/2')\n",
    "ndwi = NormalizedDifferenceIndex('NDWI', 'BANDS/1', 'BANDS/3')\n",
    "norm = EuclideanNorm('NORM','BANDS')\n",
    "\n",
    "# TASK FOR VALID MASK\n",
    "# validate pixels using SentinelHub's cloud detection mask and region of acquisition \n",
    "add_sh_valmask = AddValidDataMaskTask(SentinelHubValidData(), \n",
    "                                      'IS_VALID' # name of output mask\n",
    "                                     )\n",
    "\n",
    "# TASK FOR COUNTING VALID PIXELS\n",
    "# count number of valid observations per pixel using valid data mask \n",
    "count_val_sh = CountValid('IS_VALID', # name of existing mask\n",
    "                          'VALID_COUNT' # name of output scalar\n",
    "                         )\n",
    "\n",
    "# TASK FOR SAVING TO OUTPUT (if needed)\n",
    "path_out = os.path.join(OUTPUT_FOLDER, 'eopatches_small')\n",
    "if not os.path.isdir(path_out):\n",
    "    os.makedirs(path_out)\n",
    "save = SaveToDisk(path_out, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference map task\n",
    "\n",
    "For this example, a subset of the country-wide reference for land-use-land-cover is provided. It is available in the form of a shapefile, which contains polygons and their corresponding labels. The labels represent the following 10 classes:\n",
    "\n",
    "* lulcid = 0, name = no data\n",
    "* lulcid = 1, name = cultivated land\n",
    "* lulcid = 2, name = forest\n",
    "* lulcid = 3, name = grassland\n",
    "* lulcid = 4, name = shrubland\n",
    "* lulcid = 5, name = water\n",
    "* lulcid = 6, name = wetlands\n",
    "* lulcid = 7, name = tundra\n",
    "* lulcid = 8, name = artificial surface\n",
    "* lulcid = 9, name = bareland\n",
    "* lulcid = 10, name = snow and ice\n",
    "\n",
    "We have defined a land cover class for ease of use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LULC(Enum):\n",
    "    NO_DATA            = (0,  'No Data',            'white')\n",
    "    CULTIVATED_LAND    = (1,  'Cultivated Land',    'xkcd:lime')\n",
    "    FOREST             = (2,  'Forest',             'xkcd:darkgreen')\n",
    "    GRASSLAND          = (3,  'Grassland',          'orange')\n",
    "    SHRUBLAND          = (4,  'Shrubland',          'xkcd:tan')\n",
    "    WATER              = (5,  'Water',              'xkcd:azure')\n",
    "    WETLAND            = (6,  'Wetlands',           'xkcd:lightblue')\n",
    "    TUNDRA             = (7,  'Tundra',             'xkcd:lavender')\n",
    "    ARTIFICIAL_SURFACE = (8,  'Artificial Surface', 'crimson')\n",
    "    BARELAND           = (9,  'Bareland',           'xkcd:beige')\n",
    "    SNOW_AND_ICE       = (10, 'Snow and Ice',       'black')\n",
    "    \n",
    "    def __init__(self, val1, val2, val3):\n",
    "        self.id = val1\n",
    "        self.class_name = val2\n",
    "        self.color = val3\n",
    "        \n",
    "# example usecase\n",
    "# LULC.BARELAND.id   # return 9\n",
    "        \n",
    "# Reference colormap things\n",
    "lulc_cmap = mpl.colors.ListedColormap([entry.color for entry in LULC])\n",
    "lulc_norm = mpl.colors.BoundaryNorm(np.arange(-0.5, 11, 1), lulc_cmap.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main point of this task is to create a raster mask from the vector polygons and add it to the eopatch. With this procedure, any kind of a labeled shapefile can be transformed into a raster reference map. This result is achieved with the existing task `VectorToRaster` from the `eolearn.geometry` package. All polygons belonging to the each of the classes are separately burned to the raster mask.\n",
    "\n",
    "Land use data are public in Slovenia, you can download the full dataset [here](http://rkg.gov.si/GERK/), selecting the `Grafični podatki RABA za celo Slovenijo (shape.rar ~ 500 MB) KoordSistem: D96/TM`. The dataset is provided in the national `D96/TM` coordinate system. In order to use this dataset, you have to first convert the CRS to `UTM_33N`!\n",
    "\n",
    "The provided dataset in this notebook has already been converted for the purposes of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_cover_path = os.path.join(DATA_FOLDER, 'land_cover_subset_small', 'land_cover_subset_small.shp')\n",
    "\n",
    "land_cover_data = gpd.read_file(land_cover_path)\n",
    "\n",
    "rasterization_task = VectorToRaster(land_cover_data, (FeatureType.MASK_TIMELESS, 'LULC'),\n",
    "                                    values_column='lulcid', raster_shape=(FeatureType.MASK, 'IS_VALID'),\n",
    "                                    raster_dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the workflow\n",
    "\n",
    "All the tasks that were defined so far create and fill the EOPatches. The tasks need to be put in some order and executed one by one. This can be achieved by manually executing the tasks, or more conveniently, defining an `EOWorkflow` which does this for you.\n",
    "\n",
    "The following workflow is created and executed:\n",
    "\n",
    "1. Create EOPatches with band data\n",
    "2. Add cloud info\n",
    "3. Calculate and add NDVI, NDWI, NORM\n",
    "4. Add mask of valid pixels\n",
    "5. Add scalar feature representing the cound of valid pixels\n",
    "7. Save eopatches\n",
    "\n",
    "An EOWorkflow can be linear or more complex, but it should be acyclic. Here we will use the linear case of the EOWorkflow, available as `LinearWorkflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow = LinearWorkflow(\n",
    "    add_data,\n",
    "    add_clm,\n",
    "    ndvi,\n",
    "    ndwi,\n",
    "    norm,\n",
    "    add_sh_valmask,\n",
    "    count_val_sh,\n",
    "    rasterization_task,\n",
    "    save\n",
    ")\n",
    "\n",
    "# Let's visualize it\n",
    "workflow.dependency_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This may take some time, so go grab a cup of coffee ..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Execute the workflow\n",
    "time_interval = ['2017-01-01', '2017-12-31'] # time interval for the SH request\n",
    "\n",
    "# define additional parameters of the workflow\n",
    "execution_args = []\n",
    "for idx, bbox in enumerate(bbox_list[patchIDs]):\n",
    "    execution_args.append({\n",
    "        add_data:{'bbox': bbox, 'time_interval': time_interval},\n",
    "        save: {'eopatch_folder': 'eopatch_{}'.format(idx)}\n",
    "    })\n",
    "    \n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True, logs_folder=OUTPUT_FOLDER)\n",
    "executor.run(workers=5, multiprocess=False)\n",
    "\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the patches\n",
    "\n",
    "Let's load a single **small** EOPatch and look at the structure. By executing \n",
    "```\n",
    "EOPatch.load('./../outputs/eopatches_small/eopatch_0/')\n",
    "```\n",
    "\n",
    "We obtain the following structure:\n",
    "\n",
    "```\n",
    "EOPatch(\n",
    "  data: {\n",
    "    BANDS: numpy.ndarray(shape=(69, 337, 333, 6), dtype=float32)\n",
    "    CLP: numpy.ndarray(shape=(69, 337, 333, 1), dtype=float32)\n",
    "    NDVI: numpy.ndarray(shape=(69, 337, 333, 1), dtype=float32)\n",
    "    NDWI: numpy.ndarray(shape=(69, 337, 333, 1), dtype=float32)\n",
    "    NORM: numpy.ndarray(shape=(69, 337, 333, 1), dtype=float32)\n",
    "  }\n",
    "  mask: {\n",
    "    CLM: numpy.ndarray(shape=(69, 337, 333, 1), dtype=uint8)\n",
    "    IS_DATA: numpy.ndarray(shape=(69, 337, 333, 1), dtype=uint8)\n",
    "    IS_VALID: numpy.ndarray(shape=(69, 337, 333, 1), dtype=bool)\n",
    "  }\n",
    "  scalar: {}\n",
    "  label: {}\n",
    "  vector: {}\n",
    "  data_timeless: {}\n",
    "  mask_timeless: {\n",
    "    LULC: numpy.ndarray(shape=(337, 333, 1), dtype=uint8)\n",
    "    VALID_COUNT: numpy.ndarray(shape=(337, 333, 1), dtype=int64)\n",
    "  }\n",
    "  scalar_timeless: {}\n",
    "  label_timeless: {}\n",
    "  vector_timeless: {}\n",
    "  meta_info: {\n",
    "    maxcc: 0.8\n",
    "    service_type: 'wcs'\n",
    "    size_x: '10m'\n",
    "    size_y: '10m'\n",
    "    time_difference: datetime.timedelta(-1, 86399)\n",
    "    time_interval: <class 'list'>, length=2\n",
    "  }\n",
    "  bbox: BBox(((510157.61722214246, 5122327.229129893), (513489.214628833, 5125693.036780571)), crs=EPSG:32633)\n",
    "  timestamp: <class 'list'>, length=69\n",
    ")\n",
    "```\n",
    "\n",
    "It is possible to then access various EOPatch content via calls like:\n",
    "```\n",
    "eopatch.timestamp\n",
    "eopatch.mask['LULC']\n",
    "eopatch.data['NDVI'][0]\n",
    "eopatch.data['BANDS'][5][..., [3, 2, 1]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, all patches come from a small region, so all of them have the same dates of acquisition for at least a few dates, so we can inspect the area without interpolation at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the RGB image\n",
    "path_out = os.path.join(OUTPUT_FOLDER, 'eopatches_small')\n",
    "fig = plt.figure(figsize=(20, 20 * aspect_ratio))\n",
    "\n",
    "pbar = tqdm(total=9)\n",
    "for i in range(9):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out, i), lazy_loading=True)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(np.clip(eopatch.data['BANDS'][0][..., [2, 1, 0]] * 3.5, 0, 1))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    pbar.update(1)\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = os.path.join(OUTPUT_FOLDER, 'eopatches_small')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 20 * aspect_ratio), nrows=3, ncols=3)\n",
    "\n",
    "pbar = tqdm(total=9)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out, i), lazy_loading=True)\n",
    "    im = ax.imshow(eopatch.mask_timeless['LULC'].squeeze(), cmap=lulc_cmap, norm=lulc_norm)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    pbar.update(1)\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "cb.set_ticks([entry.id for entry in LULC])\n",
    "cb.ax.set_xticklabels([entry.class_name for entry in LULC], rotation=45, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the map of valid pixel counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = os.path.join(OUTPUT_FOLDER, 'eopatches_small')\n",
    "\n",
    "vmin, vmax = None, None\n",
    "for i in range(9):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out, i), lazy_loading=True)\n",
    "    data = eopatch.mask_timeless['VALID_COUNT'].squeeze()\n",
    "    vmin = np.min(data) if vmin is None else (np.min(data) if np.min(data) < vmin else vmin)\n",
    "    vmax = np.max(data) if vmax is None else (np.max(data) if np.max(data) > vmax else vmax)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 20 * aspect_ratio), nrows=3, ncols=3)\n",
    "    \n",
    "pbar = tqdm(total=9)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out, i), lazy_loading=True)\n",
    "    im = ax.imshow(eopatch.mask_timeless['VALID_COUNT'].squeeze(), vmin=vmin, vmax=vmax, cmap=plt.cm.inferno)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    pbar.update(1)\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial mean of NDVI\n",
    "\n",
    "Plot the mean of NDVI over all pixels in a single patch throughout the year. Filter out clouds in the mean calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = os.path.join(OUTPUT_FOLDER, 'eopatches_small')\n",
    "\n",
    "eID = 1\n",
    "eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out, eID), lazy_loading=True)\n",
    "\n",
    "ndvi = eopatch.data['NDVI'] # ndvi data cube\n",
    "mask = eopatch.mask['IS_VALID'] # mask of valid pixels\n",
    "time = np.array(eopatch.timestamp) # x axis\n",
    "t, w, h, _ = ndvi.shape \n",
    "\n",
    "ndvi_clean = ndvi.copy()\n",
    "ndvi_clean[~mask] = np.nan # set values of invalid pixels to NaN's\n",
    "\n",
    "# Calculate means, remove NaN's from means\n",
    "ndvi_mean = np.nanmean(ndvi.reshape(t, w * h).squeeze(), axis=1) \n",
    "ndvi_mean_clean = np.nanmean(ndvi_clean.reshape(t, w * h).squeeze(), axis=1)\n",
    "time_clean = time[~np.isnan(ndvi_mean_clean)]\n",
    "ndvi_mean_clean = ndvi_mean_clean[~np.isnan(ndvi_mean_clean)]\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(time_clean, ndvi_mean_clean, 's-', label = 'Mean NDVI with cloud cleaning')\n",
    "plt.plot(time, ndvi_mean, 'o-', label='Mean NDVI without cloud cleaning')\n",
    "plt.xlabel('Time', fontsize=15)\n",
    "plt.ylabel('Mean NDVI over patch', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.legend(loc=2, prop={'size': 15});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal mean of NDVI\n",
    "\n",
    "Plot the time-wise mean of NDVI for the whole region. Filter out clouds in the mean calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE CELL 2\n",
    "\n",
    "path_out = os.path.join(OUTPUT_FOLDER, 'eopatches_small')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 20 * aspect_ratio), nrows=3, ncols=3)\n",
    "    \n",
    "pbar = tqdm(total=9)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out, i), lazy_loading=True)\n",
    "    ndvi = eopatch.data['NDVI']\n",
    "    mask = eopatch.mask['IS_VALID']\n",
    "    ndvi[~mask] = np.nan\n",
    "    ndvi_mean = np.nanmean(ndvi, axis=0).squeeze()\n",
    "    im = ax.imshow(ndvi_mean, vmin=0, vmax=0.8, cmap=plt.get_cmap('YlGn'))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    pbar.update(1)\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the average cloud probability\n",
    "\n",
    "Plot te average of the cloud probability for each pixel, take the cloud mask into account.\n",
    "\n",
    "Some structures can be seen like road networks etc., indicating a bias of the cloud detector towards these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = os.path.join(OUTPUT_FOLDER, 'eopatches_small')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 20 * aspect_ratio), nrows=3, ncols=3)\n",
    "    \n",
    "pbar = tqdm(total=9)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out, i), lazy_loading=True)\n",
    "    clp = eopatch.data['CLP']\n",
    "    mask = eopatch.mask['IS_VALID']\n",
    "    clp[~mask] = np.nan\n",
    "    clp_mean = np.nanmean(clp, axis=0).squeeze()\n",
    "    im = ax.imshow(clp_mean, vmin=0.0, vmax=0.3, cmap=plt.cm.inferno)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    pbar.update(1)\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "## 5. Prepare the training data\n",
    "\n",
    "We will create a new workflow that processes the data:\n",
    "\n",
    "1. Remove too cloudy scenes\n",
    "   * Check the ratio of the valid data for each patch and for each time frame\n",
    "   * Keep only time frames with > 80 % valid coverage (no clouds)\n",
    "2. Concatenate BAND, NDVI, NDWI, NORM info into a single feature called FEATURES\n",
    "3. Perform temporal interpolation (filling gaps and resampling to the same dates)\n",
    "   * Create a task for linear interpolation in the temporal dimension\n",
    "   * Provide the cloud mask to tell the interpolating function which values to update\n",
    "4. Perform erosion\n",
    "   * This removes artefacts with a width of 1 px, and also removes the edges between polygons of different classes\n",
    "5. Random spatial sampling of the EOPatches\n",
    "   * Randomly take a subset of pixels from a patch to use in the machine learning training\n",
    "6. Split patches for training/validation\n",
    "   * Split the patches into a training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EOTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenateData(EOTask):\n",
    "    \"\"\" Task to concatenate data arrays along the last dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_name, feature_names_to_concatenate):\n",
    "        self.feature_name = feature_name\n",
    "        self.feature_names_to_concatenate = feature_names_to_concatenate\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        arrays = [eopatch.data[name] for name in self.feature_names_to_concatenate]\n",
    "\n",
    "        eopatch.add_feature(FeatureType.DATA, self.feature_name, np.concatenate(arrays, axis=-1))\n",
    "\n",
    "        return eopatch\n",
    "    \n",
    "    \n",
    "class ValidDataFractionPredicate:\n",
    "    \"\"\" Predicate that defines if a frame from EOPatch's time-series is valid or not. Frame is valid, if the \n",
    "    valid data fraction is above the specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __call__(self, array):\n",
    "        coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)\n",
    "        return coverage > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK TO LOAD EXISTING EOPATCHES\n",
    "load = LoadFromDisk(path_out)\n",
    "\n",
    "# TASK FOR CONCATENATION\n",
    "concatenate = ConcatenateData('FEATURES', ['BANDS', 'NDVI', 'NDWI', 'NORM'])\n",
    "\n",
    "# TASK FOR FILTERING OUT TOO CLOUDY SCENES\n",
    "# keep frames with > 80 % valid coverage\n",
    "valid_data_predicate = ValidDataFractionPredicate(0.8)\n",
    "filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
    "\n",
    "# TASK FOR LINEAR INTERPOLATION\n",
    "# linear interpolation of full time-series and date resampling\n",
    "resampled_range = ('2017-01-01', '2017-12-31', 16)\n",
    "linear_interp = LinearInterpolation(\n",
    "    'FEATURES', # name of field to interpolate\n",
    "    mask_feature=(FeatureType.MASK, 'IS_VALID'), # mask to be used in interpolation\n",
    "    copy_features=[(FeatureType.MASK_TIMELESS, 'LULC')], # features to keep\n",
    "    resample_range=resampled_range, # set the resampling range\n",
    "    bounds_error=False # extrapolate with NaN's\n",
    ")\n",
    "\n",
    "# TASK FOR EROSION\n",
    "# erode each class of the reference map\n",
    "erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS,'LULC','LULC_ERODED'), disk_radius=1)\n",
    "\n",
    "# TASK FOR SPATIAL SAMPLING\n",
    "# Uniformly sample about pixels from patches\n",
    "n_samples = int(4e4) # no. of pixels to sample\n",
    "ref_labels = list(range(11)) # reference labels to take into account when sampling\n",
    "spatial_sampling = PointSamplingTask(\n",
    "    n_samples=n_samples, \n",
    "    ref_mask_feature='LULC_ERODED', \n",
    "    ref_labels=ref_labels, \n",
    "    sample_features=[  # tag fields to sample\n",
    "        (FeatureType.DATA, 'FEATURES'),\n",
    "        (FeatureType.MASK_TIMELESS, 'LULC_ERODED')\n",
    "    ])\n",
    "\n",
    "path_out_sampled = os.path.join(OUTPUT_FOLDER, 'eopatches_small_sampled')\n",
    "\n",
    "if not os.path.isdir(path_out_sampled):\n",
    "    os.makedirs(path_out_sampled)\n",
    "save = SaveToDisk(path_out_sampled, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow = LinearWorkflow(\n",
    "    load,\n",
    "    concatenate,\n",
    "    filter_task,\n",
    "    linear_interp,\n",
    "    erosion,\n",
    "    spatial_sampling,\n",
    "    save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the EOWorkflow over all EOPatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "   \n",
    "execution_args = []\n",
    "for idx in range(len(patchIDs)):\n",
    "    execution_args.append({\n",
    "        load: {'eopatch_folder': 'eopatch_{}'.format(idx)},\n",
    "        save: {'eopatch_folder': 'eopatch_{}'.format(idx)}\n",
    "    })\n",
    "    \n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True, logs_folder=OUTPUT_FOLDER)\n",
    "executor.run(workers=5, multiprocess=True)\n",
    "\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model construction and training\n",
    "\n",
    "The patches are split into a train and test subset, where we take the patch with ID = 1 for testing, since it seems a good representative of the area. \n",
    "\n",
    "The test sample is hand picked because of the small set of patches, otherwise with a larged overall set, the training and testing patches should be randomly chosen.\n",
    "\n",
    "The sampled features and labels are loaded and reshaped into $n \\times m$, where $n$ represents the number of training pixels, and $m = f \\times t$ the number of all features (in this example 207), with $f$ the size of bands and band combinations (in this example 9) and $t$ the length of the resampled time-series (in this example 23)\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is used as a ML model. It is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms, used for many machine learning tasks.\n",
    "\n",
    "The default hyper-parameters are used in this example. For more info on parameter tuning, check the [ReadTheDocs](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) of the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sampled eopatches\n",
    "eopatches = []\n",
    "path_out_sampled = os.path.join(OUTPUT_FOLDER, 'eopatches_small_sampled')\n",
    "\n",
    "for i in range(9):\n",
    "    eopatches.append(EOPatch.load('{}/eopatch_{}'.format(path_out_sampled, i), lazy_loading=True))    \n",
    "\n",
    "eopatches = np.array(eopatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the train and test patch IDs\n",
    "train_ID = [0,2,3,4,5,6,7,8]\n",
    "test_ID = [1]\n",
    "\n",
    "# Set the features and the labels for train and test sets\n",
    "features_train = np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches[train_ID]])\n",
    "labels_train = np.array([eopatch.mask_timeless['LULC_ERODED_SAMPLED'] for eopatch in eopatches[train_ID]])\n",
    "features_test = np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches[test_ID]])\n",
    "labels_test = np.array([eopatch.mask_timeless['LULC_ERODED_SAMPLED'] for eopatch in eopatches[test_ID]])\n",
    "\n",
    "# get shape\n",
    "p1, t, w, h, f = features_train.shape\n",
    "p2, t, w, h, f = features_test.shape\n",
    "p = p1 + p2\n",
    "\n",
    "# reshape to n x m\n",
    "features_train = np.moveaxis(features_train, 1, 3).reshape(p1 * w * h, t * f)\n",
    "labels_train = np.moveaxis(labels_train, 1, 2).reshape(p1 * w * h, 1).squeeze()\n",
    "features_test = np.moveaxis(features_test, 1, 3).reshape(p2 * w * h, t * f)\n",
    "labels_test = np.moveaxis(labels_test, 1, 2).reshape(p2 * w * h, 1).squeeze()\n",
    "\n",
    "# remove points with no reference from training (so we dont train to recognize \"no data\")\n",
    "mask_train = labels_train == 0\n",
    "features_train = features_train[~mask_train]\n",
    "labels_train = labels_train[~mask_train]\n",
    "\n",
    "# remove points with no reference from test (so we dont validate on \"no data\", which doesn't make sense)\n",
    "mask_test = labels_test == 0\n",
    "features_test = features_test[~mask_test]\n",
    "labels_test = labels_test[~mask_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set up training classes\n",
    "labels_unique = np.unique(labels_train)\n",
    "\n",
    "# Set up the model\n",
    "model = lgb.LGBMClassifier(\n",
    "    objective='multiclass', \n",
    "    num_class=len(labels_unique), \n",
    "    metric='multi_logloss'\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# uncomment to save the model\n",
    "model_base_name = 'model_SI_LULC_smaller'\n",
    "joblib.dump(model, '{}/{}.pkl'.format(OUTPUT_FOLDER, model_base_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of the model is a crucial step in data science. All models are wrong, but some are less wrong than others, so model evaluation is important.\n",
    "\n",
    "In order to validate the model, we use the training set to predict the classes, and then compare the predicted set of labels to the \"ground truth\".\n",
    "\n",
    "Unfortunately, ground truth in the scope of EO is a term that should be taken lightly. Usually, it is not 100 % reliable due to several reasons:\n",
    "\n",
    "* Labels are determined at specific time, but land use can change (_what was once a field, may now be a house_)\n",
    "* Labels are overly generalized (_a city is an artificial surface, but it also contains parks, forests etc._)\n",
    "* Some classes can have an overlap or similar definitions (_part of a continuum, and not discrete distributions_)\n",
    "* Human error (_mistakes made when producing the reference map_)\n",
    "\n",
    "The validation is performed by evaluating various metrics, such as accuracy, precision, recall, $F_1$ score, some of which are nicely described [in this blog post](https://medium.com/greyatom/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load the model and replace with your file, usually just correct the date\n",
    "model_path = os.path.join(OUTPUT_FOLDER, 'model_SI_LULC_smaller.pkl')\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# predict the test labels\n",
    "plabels_test = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the overall accuracy (OA) and the weighted $F_1$ score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification accuracy {:.1f}%'.format(100 * metrics.accuracy_score(labels_test, plabels_test)))\n",
    "print('Classification F1-score {:.1f}%'.format(100 * metrics.f1_score(labels_test, plabels_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_1$ score, precision, and recall for each class separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_labels = np.unique(labels_test)\n",
    "class_names = [entry.class_name for entry in LULC]\n",
    "\n",
    "f1_scores = metrics.f1_score(labels_test, plabels_test, labels=class_labels, average=None)\n",
    "recall = metrics.recall_score(labels_test, plabels_test, labels=class_labels, average=None)\n",
    "precision = metrics.precision_score(labels_test, plabels_test, labels=class_labels, average=None) \n",
    "\n",
    "print('             Class              =  F1  | Recall | Precision')\n",
    "print('         --------------------------------------------------')\n",
    "for idx, lulctype in enumerate([class_names[idx] for idx in class_labels]):\n",
    "    print('         * {0:20s} = {1:2.1f} |  {2:2.1f}  | {3:2.1f}'.format(lulctype, \n",
    "                                                                         f1_scores[idx] * 100, \n",
    "                                                                         recall[idx] * 100, \n",
    "                                                                         precision[idx] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the standard and transposed Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, ylabel='True label', xlabel='Predicted label', filename=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + np.finfo(np.float).eps)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title, fontsize=20)\n",
    "    # plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(ylabel, fontsize=20)\n",
    "    plt.xlabel(xlabel, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "conf_matrix_gbm = metrics.confusion_matrix(labels_test, plabels_test)\n",
    "plot_confusion_matrix(conf_matrix_gbm, \n",
    "                      classes=[name for idx, name in enumerate(class_names) if idx in class_labels], \n",
    "                      normalize=True, \n",
    "                      ylabel='Truth (LAND COVER)', \n",
    "                      xlabel='Predicted (GBM)',\n",
    "                      title='Confusion matrix');\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "conf_matrix_gbm = metrics.confusion_matrix(plabels_test, labels_test)\n",
    "plot_confusion_matrix(conf_matrix_gbm, \n",
    "                      classes=[name for idx, name in enumerate(class_names) if idx in class_labels], \n",
    "                      normalize=True, \n",
    "                      xlabel='Truth (LAND COVER)', \n",
    "                      ylabel='Predicted (GBM)',\n",
    "                      title='Transposed Confusion matrix');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the classes the model seems to perform well. Otherwise the training sample is probably too small to make a fair assesment. \n",
    "Additional problems arise due to the unbalanced training set. The image below shows the frequency of the classes used for model training, and we see that the problematic cases are all the under-represented classes: shrubland, water, wetland, and bareland. \n",
    "\n",
    "Improving the reference map would also affect the end result, as, for example some classes are mixed up to some level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "label_ids, label_counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(range(len(label_ids)), label_counts)\n",
    "plt.xticks(range(len(label_ids)), [class_names[i] for i in label_ids], rotation=45, fontsize=20);\n",
    "plt.yticks(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves and AUC metrics\n",
    "\n",
    "Calculate precision and recall rates, draw ROC curves and calculate AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = np.unique(np.hstack([labels_test, labels_train]))\n",
    "\n",
    "scores_test = model.predict_proba(features_test)\n",
    "labels_binarized = preprocessing.label_binarize(labels_test, classes=class_labels)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for idx,lbl in enumerate(class_labels):\n",
    "    fpr[idx], tpr[idx], _ = metrics.roc_curve(labels_binarized[:, idx], scores_test[:, idx])\n",
    "    roc_auc[idx] = metrics.auc(fpr[idx], tpr[idx])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for idx,lbl in enumerate(class_labels):\n",
    "    if np.isnan(roc_auc[idx]):\n",
    "        continue\n",
    "    plt.plot(fpr[idx], tpr[idx], color=lulc_cmap.colors[lbl],\n",
    "         lw=2, label=class_names[lbl] + ' (%0.5f)' % roc_auc[idx])\n",
    "    \n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 0.99])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title('ROC Curve', fontsize=20)\n",
    "plt.legend(loc=\"center right\", prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important features\n",
    "\n",
    "Let us now check which features are most important in the above classification. The LightGBM model already contains the information about feature importances, so we only need to query them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of features\n",
    "fnames = ['B2','B3','B4','B8','B11','B12','NDVI','NDWI','NORM']\n",
    "\n",
    "# get feature importances and reshape them to dates and features\n",
    "z = model.feature_importances_.reshape((t, f))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = plt.gca()\n",
    "\n",
    "# plot the importances\n",
    "im = ax.imshow(z, aspect=0.25)\n",
    "plt.xticks(range(len(fnames)), fnames, rotation=45, fontsize=20)\n",
    "plt.yticks(range(t), ['T{}'.format(i) for i in range(t)], fontsize=20)\n",
    "plt.xlabel('Bands and band related features', fontsize=20)\n",
    "plt.ylabel('Time frames', fontsize=20)\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top') \n",
    "\n",
    "# cax = fig.add_axes([0.82, 0.125, 0.04, 0.755]) \n",
    "# plt.colorbar(im, cax=cax)\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=[ax], orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the most important features are from time frame `T1` (in this case `B2` and `NDVI`). If we look at the image for that time, we see that it is covered in snow, as shown below. It seems that snow cover provides useful information for land-use-land-cover classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the RGB image\n",
    "path_out_sampled = os.path.join(OUTPUT_FOLDER, 'eopatches_small_sampled')\n",
    "fig = plt.figure(figsize=(20, 20 * aspect_ratio))\n",
    "\n",
    "pbar = tqdm(total=9)\n",
    "for i in range(9):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out_sampled, i), lazy_loading=True)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(np.clip(eopatch.data['FEATURES'][1][..., [2, 1, 0]] * 2.5, 0, 1))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    pbar.update(1)\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check by comparing histograms for each class for `B2` and for `NDVI` on different dates. Left are the histograms for the date `T1` and for `T19` on the right. We see that for the optimal time `T1`, the feature distributions are more separatable then on a non-optimal time, so snow cover really does seem to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_t1 = np.moveaxis(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches]),\n",
    "                    1, 3)[..., 1, 0].reshape(p * h * w)\n",
    "b2_t19 = np.moveaxis(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches]),\n",
    "                     1, 3)[..., 19, 0].reshape(p * h * w)\n",
    "ndvi_t1 = np.moveaxis(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches]),\n",
    "                      1, 3)[..., 1, 6].reshape(p * h * w)\n",
    "ndvi_t19 = np.moveaxis(np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in eopatches]),\n",
    "                       1, 3)[..., 19, 6].reshape(p * h * w)\n",
    "labels = np.array([eopatch.mask_timeless['LULC_ERODED_SAMPLED'] for eopatch in eopatches]).reshape(p * h * w * 1)\n",
    "\n",
    "# remove nans\n",
    "mask = np.any([np.isnan(b2_t1), np.isnan(b2_t19), np.isnan(ndvi_t1), np.isnan(ndvi_t19), labels==0], axis=0)\n",
    "b2_t1, b2_t19, ndvi_t1, ndvi_t19, labels = [array[~mask] for array in [b2_t1, b2_t19, ndvi_t1, ndvi_t19, labels]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "plot_labels = np.unique(labels)\n",
    "plot_colors = lulc_cmap.colors\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist([b2_t1[labels == i] for i in np.unique(labels)], 100, (0.1, 0.7),histtype='step', \n",
    "         color=[plot_colors[i] for i in plot_labels]);\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('B2', fontsize=20)\n",
    "plt.title('Optimal time', fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist([b2_t19[labels == i] for i in np.unique(labels)],100,(0.1, 0.7),histtype='step', \n",
    "         color=[plot_colors[i] for i in plot_labels]);\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('B2', fontsize=20);\n",
    "plt.title('Non-optimal time', fontsize=20)\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist([ndvi_t1[labels == i] for i in plot_labels],100,(-0.4, 0.8),histtype='step', \n",
    "         color=[plot_colors[i] for i in plot_labels],\n",
    "         label=[class_names[i] for i in plot_labels]);\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('NDVI', fontsize=20)\n",
    "plt.legend(loc=1, prop={'size': 15})\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist([ndvi_t19[labels == i] for i in np.unique(labels)],100,(-0.4, 0.8),histtype='step', \n",
    "         color=[plot_colors[i] for i in plot_labels]);\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('NDVI', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of the results\n",
    "\n",
    "The model has been validated, the remaining thing is to make the prediction on the whole AOI.\n",
    "\n",
    "Here we define a workflow to make the model prediction on the existing EOPatces. The EOTask accepts the features and the names for the labels and scores. The latter is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EOTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictPatch(EOTask):\n",
    "    \"\"\"\n",
    "    Task to make model predictions on a patch. Provide the model and the feature, \n",
    "    and the output names of labels and scores (optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, features_feature, predicted_labels_name, predicted_scores_name=None):\n",
    "        self.model = model\n",
    "        self.features_feature = features_feature\n",
    "        self.predicted_labels_name = predicted_labels_name\n",
    "        self.predicted_scores_name = predicted_scores_name\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        ftrs = eopatch[self.features_feature[0]][self.features_feature[1]]\n",
    "        \n",
    "        t, w, h, f = ftrs.shape\n",
    "        ftrs = np.moveaxis(ftrs, 0, 2).reshape(w * h, t * f)\n",
    "        \n",
    "        plabels = self.model.predict(ftrs)\n",
    "        plabels = plabels.reshape(w, h)\n",
    "        plabels = plabels[..., np.newaxis]\n",
    "        eopatch.add_feature(FeatureType.MASK_TIMELESS, self.predicted_labels_name, plabels)\n",
    "        \n",
    "        if self.predicted_scores_name:\n",
    "            pscores = self.model.predict_proba(ftrs)\n",
    "            _, d = pscores.shape\n",
    "            pscores = pscores.reshape(w, h, d)\n",
    "            eopatch.add_feature(FeatureType.DATA_TIMELESS, self.predicted_scores_name, pscores)\n",
    "        \n",
    "        return eopatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tasks and the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK TO LOAD EXISTING EOPATCHES\n",
    "load = LoadFromDisk(path_out_sampled)\n",
    "\n",
    "# TASK FOR PREDICTION\n",
    "predict = PredictPatch(model, (FeatureType.DATA, 'FEATURES'), 'LBL_GBM', 'SCR_GBM')\n",
    "\n",
    "# TASK FOR SAVING\n",
    "save = SaveToDisk(str(path_out_sampled), overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "# TASK TO EXPORT TIFF\n",
    "export_tiff = ExportToTiff((FeatureType.MASK_TIMELESS, 'LBL_GBM'))\n",
    "tiff_location = os.path.join(OUTPUT_FOLDER, 'predicted_tiff')\n",
    "if not os.path.isdir(tiff_location):\n",
    "    os.makedirs(tiff_location)\n",
    "\n",
    "workflow = LinearWorkflow(\n",
    "    load,\n",
    "    predict,\n",
    "    export_tiff,\n",
    "    save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the prediction and export to GeoTIFF images\n",
    "\n",
    "Here we use the `EOExecutor` to run the workflow in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of execution arguments for each patch\n",
    "execution_args = []\n",
    "for i in range(len(patchIDs)):\n",
    "    execution_args.append(\n",
    "        {\n",
    "            load: {'eopatch_folder': 'eopatch_{}'.format(i)},\n",
    "            export_tiff: {'filename': '{}/prediction_eopatch_{}.tiff'.format(tiff_location, i)},\n",
    "            save: {'eopatch_folder': 'eopatch_{}'.format(i)}\n",
    "        }\n",
    "    )\n",
    "\n",
    "# run the executor on 2 cores\n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True, logs_folder=OUTPUT_FOLDER)\n",
    "\n",
    "# uncomment below save the logs in the current directory and produce a report!\n",
    "#executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "\n",
    "executor.run(workers=2, multiprocess=False)\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# merge with gdal_merge.py (with compression) using bash command magic\n",
    "!gdal_merge.py -o ../outputs/predicted_tiff/merged_prediction.tiff -co compress=LZW ../outputs/predicted_tiff/prediction_eopatch_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out_sampled = os.path.join(OUTPUT_FOLDER, 'eopatches_small_sampled')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 20 * aspect_ratio), nrows=3, ncols=3)\n",
    "\n",
    "pbar = tqdm(total=9)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out_sampled, i), lazy_loading=True)\n",
    "    im = ax.imshow(eopatch.mask_timeless['LBL_GBM'].squeeze(), cmap=lulc_cmap, norm=lulc_norm)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    pbar.update(1)\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "cb.set_ticks([entry.id for entry in LULC])\n",
    "cb.ax.set_xticklabels([entry.class_name for entry in LULC], rotation=45, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual inspection of patches\n",
    "\n",
    "Here is just a simple piece of code that allows a closer inspection of the predicted labels. \n",
    "\n",
    "Random subsets of patches are chosen, where prediction and ground truth are compared. For visual aid the mask of differences and the true color image are also provided.\n",
    "\n",
    "In majority of the cases, differences seem to lie on the border of different structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Reference map\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "idx = np.random.choice(range(9))\n",
    "inspect_size = 100\n",
    "\n",
    "path_out_sampled = os.path.join(OUTPUT_FOLDER, 'eopatches_small_sampled')\n",
    "eopatch = EOPatch.load('{}/eopatch_{}'.format(path_out_sampled, idx), lazy_loading=True)\n",
    "\n",
    "w, h = eopatch.mask_timeless['LULC'].squeeze().shape\n",
    "\n",
    "w_min = np.random.choice(range(w - inspect_size))\n",
    "h_min = np.random.choice(range(h - inspect_size))\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "plt.imshow(eopatch.mask_timeless['LULC'].squeeze()[w_min: w_min + inspect_size, h_min : h_min + inspect_size],\n",
    "           cmap=lulc_cmap, norm=lulc_norm)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('Ground Truth', fontsize=20)\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "plt.imshow(eopatch.mask_timeless['LBL_GBM'].squeeze()[w_min: w_min + inspect_size, h_min: h_min + inspect_size],\n",
    "           cmap=lulc_cmap, norm=lulc_norm)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('Prediction', fontsize=20)\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "mask = eopatch.mask_timeless['LBL_GBM'].squeeze() != eopatch.mask_timeless['LULC'].squeeze()\n",
    "plt.imshow(mask[w_min: w_min + inspect_size, h_min: h_min + inspect_size], cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('Difference', fontsize=20)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "image = np.clip(eopatch.data['FEATURES'][8][..., [2, 1, 0]] * 3.5, 0, 1)\n",
    "plt.imshow(image[w_min: w_min + inspect_size, h_min: h_min + inspect_size])\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('True Color', fontsize=20)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
