{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To: Land-Use-Land-Cover Prediction for Slovenia\n",
    "\n",
    "This notebook shows the steps towards constructing a machine learning pipeline for predicting the land use and land cover for the region of Republic of Slovenia. We will use satellite images obtained by ESA's Sentinel-2 to train a model and use it for prediction. The example will lead you through the whole process of creating the pipeline, with details provided at each step.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "### Requirements\n",
    "\n",
    "In order to run the example you'll need a Sentinel Hub account. If you do not have one yet, you can create a free trial account at [Sentinel Hub webpage](https://www.sentinel-hub.com/trial). If you are a researcher you can even apply for a free non-commercial account at [ESA OSEO page](https://earth.esa.int/aos/OSEO).\n",
    "\n",
    "Details on how to set up your Sentinel Hub configuration can be found [here](introduction.ipynb). \n",
    "\n",
    "### Overview\n",
    "\n",
    "#### Part 1:\n",
    "\n",
    "1. Define the Area-of-Interest (AOI):\n",
    "   * Obtain the outline of Slovenia (provided)\n",
    "   * Split into manageable smaller tiles\n",
    "   * Select a small 5x5 area for classification\n",
    "2. Use the integrated [sentinelhub-py](https://github.com/sentinel-hub/sentinelhub-py) package in order to fill the EOPatches with some content (band data, cloud masks, ...)\n",
    "   * Define the time interval (this example uses the whole year of 2019)\n",
    "3. Add additional information from band combinations (norm. vegetation index - NDVI, norm. water index - NDWI)\n",
    "4. Add a reference map (provided)\n",
    "   * Convert provided vector data to raster and add it to EOPatches\n",
    "   \n",
    "#### Part 2:\n",
    "\n",
    "5. Prepare the training data\n",
    "   * Remove too cloudy scenes\n",
    "   * Perform temporal interpolation (filling gaps and resampling to the same dates)\n",
    "   * Apply erosion \n",
    "   * Random spatial sampling of the EOPatches\n",
    "   * Split patches for training/validation\n",
    "6. Construct and train the ML model\n",
    "   * Make the prediction for each patch \n",
    "7. Validate the model\n",
    "8. Visualise the results\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, some necessary imports\n",
    "\n",
    "# Jupyter notebook related\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Built-in modules\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "from aenum import MultiValueEnum\n",
    "\n",
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine learning \n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Imports from eo-learn and sentinelhub-py\n",
    "from eolearn.core import EOTask, EOPatch, LinearWorkflow, FeatureType, OverwritePermission, \\\n",
    "    LoadTask, SaveTask, EOExecutor, ExtractBandsTask, MergeFeatureTask\n",
    "from eolearn.io import SentinelHubInputTask, VectorImportTask, ExportToTiffTask\n",
    "from eolearn.mask import AddValidDataMaskTask\n",
    "from eolearn.geometry import VectorToRasterTask, PointSamplingTask, ErosionTask\n",
    "from eolearn.features import LinearInterpolationTask, SimpleFilterTask, NormalizedDifferenceIndexTask\n",
    "from sentinelhub import UtmZoneSplitter, BBox, CRS, DataCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "## 1. Define the Area-of-Interest (AOI):\n",
    "\n",
    "* A geographical shape of Slovenia was taken from [Natural Earth database](http://www.naturalearthdata.com/downloads/10m-cultural-vectors/) and a buffer of 500 m was applied. The shape is available in repository: `data/svn_border.geojson`\n",
    "* Convert it to selected CRS: taken to be the CRS of central UTM tile (UTM_33N)\n",
    "* Split it into smaller, manageable, non-overlapping rectangular tiles\n",
    "* Run classification on a selected 5x5 area\n",
    "\n",
    "Be sure that your choice of CRS is the same as the CRS of your reference data.\n",
    "\n",
    "In the case that you are having problems with empty data being downloaded, try changing the CRS to something that suits the location of the AOI better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get country boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where data for running the notebook is stored\n",
    "DATA_FOLDER = os.path.join('..', 'data')\n",
    "# Locations for collected data and intermediate results\n",
    "EOPATCH_FOLDER = os.path.join('.', 'eopatches')\n",
    "EOPATCH_SAMPLES_FOLDER = os.path.join('.', 'eopatches_sampled')\n",
    "RESULTS_FOLDER = os.path.join('.', 'results')\n",
    "os.makedirs(EOPATCH_FOLDER, exist_ok=True)\n",
    "os.makedirs(EOPATCH_SAMPLES_FOLDER, exist_ok=True)\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "\n",
    "# Load geojson file\n",
    "country = gpd.read_file(os.path.join(DATA_FOLDER, 'svn_border.geojson'))\n",
    "# Add 500m buffer to secure sufficient data near border\n",
    "country = country.buffer(500)\n",
    "\n",
    "# Get the country's shape in polygon format\n",
    "country_shape = country.geometry.values[0]\n",
    "\n",
    "# Plot country\n",
    "country.plot()\n",
    "plt.axis('off');\n",
    "\n",
    "# Print size\n",
    "country_width = country_shape.bounds[2] - country_shape.bounds[0]\n",
    "country_height = country_shape.bounds[3] - country_shape.bounds[1]\n",
    "print(f'Dimension of the area is {country_width:.0f} x {country_height:.0f} m2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to smaller tiles and choose a 5x5 area\n",
    "\n",
    "The splitting choice depends on the available resources of your computer. An EOPatch with a size of has around 500 x 500 pixels at 10 meter resolution has a size ob about ~1 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter to obtain a list of bboxes with 5km sides\n",
    "bbox_splitter = UtmZoneSplitter([country_shape], country.crs, 5000)\n",
    "\n",
    "bbox_list = np.array(bbox_splitter.get_bbox_list())\n",
    "info_list = np.array(bbox_splitter.get_info_list())\n",
    "\n",
    "# Prepare info of selected EOPatches\n",
    "geometry = [Polygon(bbox.get_polygon()) for bbox in bbox_list]\n",
    "idxs = [info['index'] for info in info_list]\n",
    "idxs_x = [info['index_x'] for info in info_list]\n",
    "idxs_y = [info['index_y'] for info in info_list]\n",
    "\n",
    "bbox_gdf = gpd.GeoDataFrame({'index': idxs, 'index_x': idxs_x, 'index_y': idxs_y}, \n",
    "                            crs=country.crs, geometry=geometry)\n",
    "\n",
    "# select a 5x5 area (id of center patch)\n",
    "ID = 616\n",
    "\n",
    "# Obtain surrounding 5x5 patches\n",
    "patchIDs = []\n",
    "for idx, (bbox, info) in enumerate(zip(bbox_list, info_list)):\n",
    "    if (abs(info['index_x'] - info_list[ID]['index_x']) <= 2 and\n",
    "        abs(info['index_y'] - info_list[ID]['index_y']) <= 2):\n",
    "        patchIDs.append(idx)\n",
    "\n",
    "# Check if final size is 5x5\n",
    "if len(patchIDs) != 5*5:\n",
    "    print('Warning! Use a different central patch ID, this one is on the border.')\n",
    "    \n",
    "# Change the order of the patches (useful for plotting)\n",
    "patchIDs = np.transpose(np.fliplr(np.array(patchIDs).reshape(5, 5))).ravel()\n",
    "\n",
    "# Save to shapefile\n",
    "shapefile_name = 'grid_slovenia_500x500.gpkg'\n",
    "bbox_gdf.to_file(os.path.join(RESULTS_FOLDER, shapefile_name), driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display bboxes over country\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "ax.set_title('Selected 5x5 tiles from Slovenia', fontsize=25)\n",
    "country.plot(ax=ax, facecolor='w', edgecolor='b', alpha=0.5)\n",
    "bbox_gdf.plot(ax=ax, facecolor='w', edgecolor='r', alpha=0.5)\n",
    "\n",
    "for bbox, info in zip(bbox_list, info_list):\n",
    "    geo = bbox.geometry\n",
    "    ax.text(geo.centroid.x, geo.centroid.y, info['index'], ha='center', va='center')\n",
    "    \n",
    "# Mark bboxes of selected area\n",
    "bbox_gdf[bbox_gdf.index.isin(patchIDs)].plot(ax=ax, facecolor='g', edgecolor='r', alpha=0.5)\n",
    "\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. - 4. Fill EOPatches with data:\n",
    "\n",
    "Now it's time to create EOPatches and fill them with Sentinel-2 data using Sentinel Hub services. We will add the following data to each EOPatch:\n",
    "\n",
    "* L1C custom list of bands [B02, B03, B04, B08, B11, B12], which corresponds to [B, G, R, NIR, SWIR1, SWIR2] wavelengths.\n",
    "\n",
    "* SentinelHub's cloud mask\n",
    "\n",
    "Additionally, we will add:\n",
    "\n",
    "* Calculated NDVI, NDWI, and NDBI information\n",
    "\n",
    "* A mask of validity, based on acquired data from Sentinel and cloud coverage. Valid pixel is if:\n",
    "   \n",
    "    1. IS_DATA == True\n",
    "    2. CLOUD_MASK == 0 (1 indicates cloudy pixels and 255 indicates `NO_DATA`)\n",
    "\n",
    "An EOPatch is created and manipulated using EOTasks, which are chained in an EOWorkflow. In this example the final workflow is executed on all patches, which are saved to the specified directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some needed custom EOTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentinelHubValidData:\n",
    "    \"\"\"\n",
    "    Combine Sen2Cor's classification map with `IS_DATA` to define a `VALID_DATA_SH` mask\n",
    "    The SentinelHub's cloud mask is asumed to be found in eopatch.mask['CLM']\n",
    "    \"\"\"\n",
    "    def __call__(self, eopatch):      \n",
    "        return eopatch.mask['IS_DATA'].astype(bool) & np.logical_not(eopatch.mask['CLM'].astype(bool))\n",
    "    \n",
    "class AddValidCountTask(EOTask):   \n",
    "    \"\"\"\n",
    "    The task counts number of valid observations in time-series and stores the results in the timeless mask.\n",
    "    \"\"\"\n",
    "    def __init__(self, count_what, feature_name):\n",
    "        self.what = count_what\n",
    "        self.name = feature_name\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        eopatch[(FeatureType.MASK_TIMELESS, self.name)] = np.count_nonzero(eopatch.mask[self.what], axis=0)\n",
    "        return eopatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the workflow tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAND DATA\n",
    "# Add a request for S2 bands.\n",
    "# Here we also do a simple filter of cloudy scenes (on tile level).\n",
    "# The s2cloudless masks and probabilities are requested via additional data.\n",
    "band_names = ['B02', 'B03', 'B04', 'B08', 'B11', 'B12']\n",
    "add_data = SentinelHubInputTask(\n",
    "    bands_feature=(FeatureType.DATA, 'BANDS'),\n",
    "    bands = band_names,\n",
    "    resolution=10,\n",
    "    maxcc=0.8,\n",
    "    time_difference=datetime.timedelta(minutes=120),\n",
    "    data_collection=DataCollection.SENTINEL2_L1C,\n",
    "    additional_data=[(FeatureType.MASK, 'dataMask', 'IS_DATA'),\n",
    "                     (FeatureType.MASK, 'CLM'),\n",
    "                     (FeatureType.DATA, 'CLP')],\n",
    "    max_threads=5\n",
    ")\n",
    "\n",
    "\n",
    "# CALCULATING NEW FEATURES\n",
    "# NDVI: (B08 - B04)/(B08 + B04)\n",
    "# NDWI: (B03 - B08)/(B03 + B08)\n",
    "# NDBI: (B11 - B08)/(B11 + B08)\n",
    "ndvi = NormalizedDifferenceIndexTask((FeatureType.DATA, 'BANDS'), (FeatureType.DATA, 'NDVI'), \n",
    "                                     [band_names.index('B08'), band_names.index('B04')])\n",
    "ndwi = NormalizedDifferenceIndexTask((FeatureType.DATA, 'BANDS'), (FeatureType.DATA, 'NDWI'), \n",
    "                                     [band_names.index('B03'), band_names.index('B08')])\n",
    "ndbi = NormalizedDifferenceIndexTask((FeatureType.DATA, 'BANDS'), (FeatureType.DATA, 'NDBI'), \n",
    "                                     [band_names.index('B11'), band_names.index('B08')])\n",
    "\n",
    "\n",
    "\n",
    "# VALIDITY MASK\n",
    "# Validate pixels using SentinelHub's cloud detection mask and region of acquisition \n",
    "add_sh_validmask = AddValidDataMaskTask(SentinelHubValidData(), 'IS_VALID')\n",
    "\n",
    "# COUNTING VALID PIXELS\n",
    "# Count the number of valid observations per pixel using valid data mask \n",
    "add_valid_count = AddValidCountTask('IS_VALID', 'VALID_COUNT')\n",
    "\n",
    "# SAVING TO OUTPUT (if needed)\n",
    "save = SaveTask(EOPATCH_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Help! I prefer to calculate cloud masks of my own!\n",
    "\n",
    "If you wish to calculate `s2cloudless` masks and probabilities (almost) from scratch, you can do this by using the following two EOTasks instead of the first one above\n",
    "\n",
    "```python\n",
    "band_names = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
    "add_data = SentinelHubInputTask(\n",
    "    bands_feature=(FeatureType.DATA, 'BANDS'),\n",
    "    bands = band_names,\n",
    "    resolution=10,\n",
    "    maxcc=0.8,\n",
    "    time_difference=datetime.timedelta(minutes=120),\n",
    "    data_collection=DataCollection.SENTINEL2_L1C,\n",
    "    additional_data=[(FeatureType.MASK, 'dataMask', 'IS_DATA')],\n",
    ")\n",
    "\n",
    "add_clm = CloudMaskTask(data_feature='BANDS',\n",
    "                        all_bands=True,\n",
    "                        processing_resolution=160,\n",
    "                        mono_features=('CLP', 'CLM'),\n",
    "                        mask_feature=None,\n",
    "                        average_over=16,\n",
    "                        dilation_size=8)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference map task\n",
    "\n",
    "For this example, a subset of the country-wide reference for land-use-land-cover is provided. It is available in the form of a geopackage, which contains polygons and their corresponding labels. The labels represent the following 10 classes:\n",
    "\n",
    "* lulcid = 0, name = no data\n",
    "* lulcid = 1, name = cultivated land\n",
    "* lulcid = 2, name = forest\n",
    "* lulcid = 3, name = grassland\n",
    "* lulcid = 4, name = shrubland\n",
    "* lulcid = 5, name = water\n",
    "* lulcid = 6, name = wetlands\n",
    "* lulcid = 7, name = tundra\n",
    "* lulcid = 8, name = artificial surface\n",
    "* lulcid = 9, name = bareland\n",
    "* lulcid = 10, name = snow and ice\n",
    "\n",
    "We have defined a land cover enum class for ease of use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LULC(MultiValueEnum):\n",
    "    \"\"\" Enum class containing basic LULC types\n",
    "    \"\"\"\n",
    "    NO_DATA            = 'No Data',            0,  '#ffffff'\n",
    "    CULTIVATED_LAND    = 'Cultivated Land',    1,  '#ffff00'\n",
    "    FOREST             = 'Forest',             2,  '#054907'\n",
    "    GRASSLAND          = 'Grassland',          3,  '#ffa500'\n",
    "    SHRUBLAND          = 'Shrubland',          4,  '#806000'\n",
    "    WATER              = 'Water',              5,  '#069af3'\n",
    "    WETLAND            = 'Wetlands',           6,  '#95d0fc'\n",
    "    TUNDRA             = 'Tundra',             7,  '#967bb6'\n",
    "    ARTIFICIAL_SURFACE = 'Artificial Surface', 8,  '#dc143c'\n",
    "    BARELAND           = 'Bareland',           9,  '#a6a6a6'\n",
    "    SNOW_AND_ICE       = 'Snow and Ice',       10, '#000000'\n",
    "    \n",
    "    @property\n",
    "    def id(self):\n",
    "        \"\"\" Returns an ID of an enum type\n",
    "\n",
    "        :return: An ID\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.values[1]\n",
    "\n",
    "    @property\n",
    "    def color(self):\n",
    "        \"\"\" Returns class color\n",
    "\n",
    "        :return: A color in hexadecimal representation\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        return self.values[2]\n",
    "\n",
    "\n",
    "def get_bounds_from_ids(ids):\n",
    "    bounds = []\n",
    "    for i in range(len(ids)):\n",
    "        if i < len(ids) - 1:\n",
    "            if i == 0:\n",
    "                diff = (ids[i + 1] - ids[i]) / 2\n",
    "                bounds.append(ids[i] - diff)\n",
    "            diff = (ids[i + 1] - ids[i]) / 2\n",
    "            bounds.append(ids[i] + diff)\n",
    "        else:\n",
    "            diff = (ids[i] - ids[i - 1]) / 2\n",
    "            bounds.append(ids[i] + diff)\n",
    "    return bounds\n",
    "    \n",
    "\n",
    "# Reference colormap things\n",
    "lulc_bounds = get_bounds_from_ids([x.id for x in LULC])\n",
    "lulc_cmap = ListedColormap([x.color for x in LULC], name=\"lulc_cmap\")\n",
    "lulc_norm = BoundaryNorm(lulc_bounds, lulc_cmap.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main point of this task is to create a raster mask from the vector polygons and add it to the eopatch. With this procedure, any kind of a labeled shapefile can be transformed into a raster reference map. This result is achieved with the existing task `VectorToRaster` from the `eolearn.geometry` package. All polygons belonging to the each of the classes are separately burned to the raster mask.\n",
    "\n",
    "Land use data are public in Slovenia, you can use the provided partial dataset for this example, or download the full dataset (if you want to upscale the project) [from our bucket](http://eo-learn.sentinel-hub.com/). The datasets have already been pre-processed for the purposes of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_use_ref_path = os.path.join(DATA_FOLDER, 'land_use_10class_reference_slovenia_partial.gpkg')\n",
    "vector_feature = FeatureType.VECTOR_TIMELESS, 'LULC_REFERENCE'\n",
    "\n",
    "vector_import_task = VectorImportTask(vector_feature, land_use_ref_path)\n",
    "\n",
    "rasterization_task = VectorToRasterTask(\n",
    "    vector_feature,\n",
    "    (FeatureType.MASK_TIMELESS, 'LULC'),\n",
    "    values_column='lulcid',\n",
    "    raster_shape=(FeatureType.MASK, 'IS_DATA'),\n",
    "    raster_dtype=np.uint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the workflow\n",
    "\n",
    "All the tasks that were defined so far create and fill the EOPatches. The tasks need to be put in some order and executed one by one. This can be achieved by manually executing the tasks, or more conveniently, defining an `EOWorkflow` which does this for you.\n",
    "\n",
    "The following workflow is created and executed:\n",
    "\n",
    "1. Create EOPatches with band and cloud data\n",
    "3. Calculate and add NDVI, NDWI, NORM\n",
    "4. Add mask of valid pixels\n",
    "5. Add scalar feature representing the count of valid pixels\n",
    "7. Save eopatches\n",
    "\n",
    "An EOWorkflow can be linear or more complex, but it should be acyclic. Here we will use the linear case of the EOWorkflow, available as `LinearWorkflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow = LinearWorkflow(\n",
    "    add_data,\n",
    "    ndvi,\n",
    "    ndwi,\n",
    "    ndbi,\n",
    "    add_sh_validmask,\n",
    "    add_valid_count,\n",
    "    vector_import_task,\n",
    "    rasterization_task,\n",
    "    save\n",
    ")\n",
    "\n",
    "# Let's visualize it\n",
    "workflow.dependency_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This may take some time, so go grab a cup of coffee ..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Time interval for the SH request\n",
    "time_interval = ['2019-01-01', '2019-12-31'] \n",
    "\n",
    "# Define additional parameters of the workflow\n",
    "execution_args = []\n",
    "for idx, bbox in enumerate(bbox_list[patchIDs]):\n",
    "    execution_args.append({\n",
    "        add_data: {'bbox': bbox, 'time_interval': time_interval},\n",
    "        save: {'eopatch_folder': f'eopatch_{idx}'}\n",
    "    })\n",
    "    \n",
    "# Execute the workflow\n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=5, multiprocess=True)\n",
    "\n",
    "executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(f'Execution failed EOPatches with IDs:\\n{failed_ids}\\n'\n",
    "                       f'For more info check report at {executor.get_report_filename()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the patches\n",
    "\n",
    "Let's load a single EOPatch and look at the structure. By executing \n",
    "```\n",
    "EOPatch.load('./eopatches/eopatch_0/')\n",
    "```\n",
    "\n",
    "We obtain the following structure:\n",
    "\n",
    "```\n",
    "EOPatch(\n",
    "  data: {\n",
    "    BANDS: numpy.ndarray(shape=(48, 500, 500, 6), dtype=float32)\n",
    "    CLP: numpy.ndarray(shape=(48, 500, 500, 1), dtype=uint8)\n",
    "    NDBI: numpy.ndarray(shape=(48, 500, 500, 1), dtype=float32)\n",
    "    NDVI: numpy.ndarray(shape=(48, 500, 500, 1), dtype=float32)\n",
    "    NDWI: numpy.ndarray(shape=(48, 500, 500, 1), dtype=float32)\n",
    "  }\n",
    "  mask: {\n",
    "    CLM: numpy.ndarray(shape=(48, 500, 500, 1), dtype=uint8)\n",
    "    IS_DATA: numpy.ndarray(shape=(48, 500, 500, 1), dtype=uint8)\n",
    "    IS_VALID: numpy.ndarray(shape=(48, 500, 500, 1), dtype=bool)\n",
    "  }\n",
    "  scalar: {}\n",
    "  label: {}\n",
    "  vector: {}\n",
    "  data_timeless: {}\n",
    "  mask_timeless: {\n",
    "    LULC: numpy.ndarray(shape=(500, 500, 1), dtype=uint8)\n",
    "    VALID_COUNT: numpy.ndarray(shape=(500, 500, 1), dtype=int64)\n",
    "  }\n",
    "  scalar_timeless: {}\n",
    "  label_timeless: {}\n",
    "  vector_timeless: {\n",
    "    LULC_REFERENCE: geopandas.GeoDataFrame(columns=['RABA_PID', 'RABA_ID', 'VIR', 'AREA', 'STATUS', 'D_OD', 'lulcid', 'lulcname', 'geometry'], length=1145, crs=EPSG:32633)\n",
    "  }\n",
    "  meta_info: {\n",
    "    maxcc: 0.8\n",
    "    size_x: 500\n",
    "    size_y: 500\n",
    "    time_difference: datetime.timedelta(seconds=7200)\n",
    "    time_interval: ('2019-01-01T00:00:00', '2019-12-31T23:59:59')\n",
    "  }\n",
    "  bbox: BBox(((500000.0, 5135000.0), (505000.0, 5140000.0)), crs=CRS('32633'))\n",
    "  timestamp: [datetime.datetime(2019, 1, 1, 10, 7, 42), ..., datetime.datetime(2019, 12, 7, 10, 7, 45)], length=48\n",
    ")\n",
    "```\n",
    "\n",
    "It is possible to then access various EOPatch content via calls like:\n",
    "```\n",
    "eopatch.timestamp\n",
    "eopatch.mask['LULC']\n",
    "eopatch.data['NDVI'][0]\n",
    "eopatch.data['BANDS'][5][..., [3, 2, 1]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the `maxcc` filtering, not all patches have the same amount of timestamps. \n",
    "\n",
    "Let's select a date and draw the closest timestamp for each eopatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the RGB images\n",
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n",
    "\n",
    "date = datetime.datetime(2019, 7, 1)\n",
    "\n",
    "for i in tqdm(range(len(patchIDs))):\n",
    "    eopatch_path = os.path.join(EOPATCH_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)\n",
    "    \n",
    "    dates = np.array([timestamp.replace(tzinfo=None) for timestamp in eopatch.timestamp])\n",
    "    closest_date_id = np.argsort(abs(date-dates))[0]\n",
    "    \n",
    "    ax = axs[i//5][i%5]\n",
    "    ax.imshow(np.clip(eopatch.data['BANDS'][closest_date_id][..., [2, 1, 0]] * 3.5, 0, 1))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))\n",
    "\n",
    "for i in tqdm(range(len(patchIDs))):\n",
    "    eopatch_path = os.path.join(EOPATCH_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)\n",
    "    \n",
    "    ax = axs[i//5][i%5]\n",
    "    im = ax.imshow(eopatch.mask_timeless['LULC'].squeeze(), cmap=lulc_cmap, norm=lulc_norm)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('auto')\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "cb.set_ticks([entry.id for entry in LULC])\n",
    "cb.ax.set_xticklabels([entry.name for entry in LULC], rotation=45, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the map of valid pixel counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min and max counts of valid data per pixel\n",
    "vmin, vmax = None, None\n",
    "for i in range(len(patchIDs)):\n",
    "    eopatch_path = os.path.join(EOPATCH_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)\n",
    "    data = eopatch.mask_timeless['VALID_COUNT'].squeeze()\n",
    "    vmin = np.min(data) if vmin is None else (np.min(data) if np.min(data) < vmin else vmin)\n",
    "    vmax = np.max(data) if vmax is None else (np.max(data) if np.max(data) > vmax else vmax)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))\n",
    "    \n",
    "for i in tqdm(range(len(patchIDs))):\n",
    "    eopatch_path = os.path.join(EOPATCH_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)\n",
    "    ax = axs[i//5][i%5]\n",
    "    im = ax.imshow(eopatch.mask_timeless['VALID_COUNT'].squeeze(), vmin=vmin, vmax=vmax, cmap=plt.cm.inferno)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial mean of NDVI\n",
    "\n",
    "Plot the mean of NDVI over all pixels in a selected patch throughout the year. Filter out clouds in the mean calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eID = 16\n",
    "eopatch = EOPatch.load(os.path.join(EOPATCH_FOLDER, f'eopatch_{i}'), lazy_loading=True)\n",
    "\n",
    "ndvi = eopatch.data['NDVI']\n",
    "mask = eopatch.mask['IS_VALID']\n",
    "time = np.array(eopatch.timestamp)\n",
    "t, w, h, _ = ndvi.shape\n",
    "\n",
    "ndvi_clean = ndvi.copy()\n",
    "ndvi_clean[~mask] = np.nan  # Set values of invalid pixels to NaN's\n",
    "\n",
    "# Calculate means, remove NaN's from means\n",
    "ndvi_mean = np.nanmean(ndvi.reshape(t, w * h), axis=1)\n",
    "ndvi_mean_clean = np.nanmean(ndvi_clean.reshape(t, w * h), axis=1)\n",
    "time_clean = time[~np.isnan(ndvi_mean_clean)]\n",
    "ndvi_mean_clean = ndvi_mean_clean[~np.isnan(ndvi_mean_clean)]\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.plot(time_clean, ndvi_mean_clean, 's-', label = 'Mean NDVI with cloud cleaning')\n",
    "plt.plot(time, ndvi_mean, 'o-', label='Mean NDVI without cloud cleaning')\n",
    "plt.xlabel('Time', fontsize=15)\n",
    "plt.ylabel('Mean NDVI over patch', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.legend(loc=2, prop={'size': 15});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal mean of NDVI\n",
    "\n",
    "Plot the time-wise mean of NDVI for the whole region. Filter out clouds in the mean calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))\n",
    "    \n",
    "for i in tqdm(range(len(patchIDs))):\n",
    "    eopatch_path = os.path.join(EOPATCH_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)\n",
    "    ndvi = eopatch.data['NDVI']\n",
    "    mask = eopatch.mask['IS_VALID']\n",
    "    ndvi[~mask] = np.nan\n",
    "    ndvi_mean = np.nanmean(ndvi, axis=0).squeeze()\n",
    "    \n",
    "    ax = axs[i//5][i%5]\n",
    "    im = ax.imshow(ndvi_mean, vmin=0, vmax=0.8, cmap=plt.get_cmap('YlGn'))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the average cloud probability\n",
    "\n",
    "Plot te average of the cloud probability for each pixel, take the cloud mask into account.\n",
    "\n",
    "Some structures can be seen like road networks etc., indicating a bias of the cloud detector towards these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))\n",
    "\n",
    "for i in tqdm(range(len(patchIDs))):\n",
    "    eopatch_path = os.path.join(EOPATCH_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)\n",
    "    clp = eopatch.data['CLP'].astype(float)/255\n",
    "    mask = eopatch.mask['IS_VALID']\n",
    "    clp[~mask] = np.nan\n",
    "    clp_mean = np.nanmean(clp, axis=0).squeeze()\n",
    "    \n",
    "    ax = axs[i//5][i%5]\n",
    "    im = ax.imshow(clp_mean, vmin=0.0, vmax=0.3, cmap=plt.cm.inferno)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "## 5. Prepare the training data\n",
    "\n",
    "We will create a new workflow that processes the data:\n",
    "\n",
    "1. Remove too cloudy scenes\n",
    "   * Check the ratio of the valid data for each patch and for each time frame\n",
    "   * Keep only time frames with > 80 % valid coverage (no clouds)\n",
    "2. Concatenate BAND, NDVI, NDWI, NDBI info into a single feature called FEATURES\n",
    "3. Perform temporal interpolation (filling gaps and resampling to the same dates)\n",
    "   * Create a task for linear interpolation in the temporal dimension\n",
    "   * Provide the cloud mask to tell the interpolating function which values to update\n",
    "4. Perform erosion\n",
    "   * This removes artefacts with a width of 1 px, and also removes the edges between polygons of different classes\n",
    "5. Random spatial sampling of the EOPatches\n",
    "   * Randomly take a subset of pixels from a patch to use in the machine learning training\n",
    "6. Split patches for training/validation\n",
    "   * Split the patches into a training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EOTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataFractionPredicate:\n",
    "    \"\"\" Predicate that defines if a frame from EOPatch's time-series is valid or not. Frame is valid if the \n",
    "    valid data fraction is above the specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __call__(self, array):\n",
    "        coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)\n",
    "        return coverage > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EXISTING EOPATCHES\n",
    "load = LoadTask(EOPATCH_FOLDER)\n",
    "\n",
    "# FEATURE CONCATENATION\n",
    "concatenate = MergeFeatureTask({FeatureType.DATA: ['BANDS', 'NDVI', 'NDWI', 'NDBI']},\n",
    "                               (FeatureType.DATA, 'FEATURES'))\n",
    "\n",
    "# FILTER OUT CLOUDY SCENES\n",
    "# Keep frames with > 80% valid coverage\n",
    "valid_data_predicate = ValidDataFractionPredicate(0.8)\n",
    "filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
    "\n",
    "# LINEAR TEMPORAL INTERPOLATION\n",
    "# linear interpolation of full time-series and date resampling\n",
    "resampled_range = ('2019-01-01', '2019-12-31', 15)\n",
    "linear_interp = LinearInterpolationTask(\n",
    "    'FEATURES', # name of field to interpolate\n",
    "    mask_feature=(FeatureType.MASK, 'IS_VALID'), # mask to be used in interpolation\n",
    "    copy_features=[(FeatureType.MASK_TIMELESS, 'LULC')], # features to keep\n",
    "    resample_range=resampled_range,\n",
    ")\n",
    "\n",
    "# EROSION\n",
    "# erode each class of the reference map\n",
    "erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS,'LULC','LULC_ERODED'), disk_radius=1)\n",
    "\n",
    "# SPATIAL SAMPLING\n",
    "# Uniformly sample pixels from patches\n",
    "n_samples = 125000 # half of pixels\n",
    "lulc_type_ids = [lulc_type.id for lulc_type in LULC] \n",
    "\n",
    "spatial_sampling = PointSamplingTask(\n",
    "    n_samples=n_samples, \n",
    "    ref_mask_feature='LULC_ERODED', \n",
    "    ref_labels=lulc_type_ids,\n",
    "    sample_features=[(FeatureType.DATA, 'FEATURES'), (FeatureType.MASK_TIMELESS, 'LULC_ERODED')],\n",
    "    )\n",
    "\n",
    "save = SaveTask(EOPATCH_SAMPLES_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow = LinearWorkflow(\n",
    "    load,\n",
    "    concatenate,\n",
    "    filter_task,\n",
    "    linear_interp,\n",
    "    erosion,\n",
    "    spatial_sampling,\n",
    "    save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the EOWorkflow over all EOPatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "   \n",
    "execution_args = []\n",
    "for idx in range(len(patchIDs)):\n",
    "    execution_args.append({\n",
    "        load: {'eopatch_folder': f'eopatch_{idx}'},\n",
    "        spatial_sampling: {'seed': 42},\n",
    "        save: {'eopatch_folder': f'eopatch_{idx}'}\n",
    "    })\n",
    "    \n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=5, multiprocess=True)\n",
    "\n",
    "executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(f'Execution failed EOPatches with IDs:\\n{failed_ids}\\n'\n",
    "                       f'For more info check report at {executor.get_report_filename()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model construction and training\n",
    "\n",
    "The patches are split into a train and test subset, where we split the patches for training and testing.\n",
    "\n",
    "The test sample is hand picked because of the small set of patches, otherwise with a larged overall set, the training and testing patches should be randomly chosen.\n",
    "\n",
    "The sampled features and labels are loaded and reshaped into $n \\times m$, where $n$ represents the number of training pixels, and $m = f \\times t$ the number of all features (in this example 216), with $f$ the size of bands and band combinations (in this example 9) and $t$ the length of the resampled time-series (in this example 25)\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is used as a ML model. It is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms, used for many machine learning tasks.\n",
    "\n",
    "The default hyper-parameters are used in this example. For more info on parameter tuning, check the [ReadTheDocs](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html) of the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled eopatches\n",
    "sampled_eopatches = []\n",
    "\n",
    "for i in range(len(patchIDs)):\n",
    "    sample_path = os.path.join(EOPATCH_SAMPLES_FOLDER, f'eopatch_{i}')\n",
    "    sampled_eopatches.append(EOPatch.load(sample_path, lazy_loading=True))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the train and test patch IDs, take 80 % for train\n",
    "test_ID = [0, 8, 16, 19, 20]\n",
    "test_eopatches = [sampled_eopatches[i] for i in test_ID]\n",
    "train_ID = [i for i in range(len(patchIDs)) if i not in test_ID]\n",
    "train_eopatches = [sampled_eopatches[i] for i in train_ID]\n",
    "\n",
    "# Set the features and the labels for train and test sets\n",
    "features_train = np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in train_eopatches])\n",
    "labels_train = np.array([eopatch.mask_timeless['LULC_ERODED_SAMPLED'] for eopatch in train_eopatches])\n",
    "\n",
    "features_test = np.array([eopatch.data['FEATURES_SAMPLED'] for eopatch in test_eopatches])\n",
    "labels_test = np.array([eopatch.mask_timeless['LULC_ERODED_SAMPLED'] for eopatch in test_eopatches])\n",
    "\n",
    "# Get shape\n",
    "p1, t, w, h, f = features_train.shape\n",
    "p2, t, w, h, f = features_test.shape\n",
    "p = p1 + p2\n",
    "\n",
    "# Reshape to n x m\n",
    "features_train = np.moveaxis(features_train, 1, 3).reshape(p1 * w * h, t * f)\n",
    "labels_train = np.moveaxis(labels_train, 1, 2).reshape(p1 * w * h)\n",
    "features_test = np.moveaxis(features_test, 1, 3).reshape(p2 * w * h, t * f)\n",
    "labels_test = np.moveaxis(labels_test, 1, 2).reshape(p2 * w * h)\n",
    "\n",
    "# Remove points with no reference from training (so we dont train to recognize \"no data\")\n",
    "mask_train = labels_train == 0\n",
    "features_train = features_train[~mask_train]\n",
    "labels_train = labels_train[~mask_train]\n",
    "\n",
    "# Remove points with no reference from test (so we dont validate on \"no data\", which doesn't make sense)\n",
    "mask_test = labels_test == 0\n",
    "features_test = features_test[~mask_test]\n",
    "labels_test = labels_test[~mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set up training classes\n",
    "labels_unique = np.unique(labels_train)\n",
    "\n",
    "# Set up the model\n",
    "model = lgb.LGBMClassifier(\n",
    "    objective='multiclass', \n",
    "    num_class=len(labels_unique), \n",
    "    metric='multi_logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, os.path.join(RESULTS_FOLDER, 'model_SI_LULC.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of the model is a crucial step in data science. All models are wrong, but some are less wrong than others, so model evaluation is important.\n",
    "\n",
    "In order to validate the model, we use the training set to predict the classes, and then compare the predicted set of labels to the \"ground truth\".\n",
    "\n",
    "Unfortunately, ground truth in the scope of EO is a term that should be taken lightly. Usually, it is not 100 % reliable due to several reasons:\n",
    "\n",
    "* Labels are determined at specific time, but land use can change (*what was once a field, may now be a house*)\n",
    "* Labels are overly generalized (*a city is an artificial surface, but it also contains parks, forests etc.*)\n",
    "* Some classes can have an overlap or similar definitions (*part of a continuum, and not discrete distributions*)\n",
    "* Human error (*mistakes made when producing the reference map*)\n",
    "\n",
    "The validation is performed by evaluating various metrics, such as accuracy, precision, recall, $F_1$ score, some of which are nicely described [in this blog post](https://medium.com/greyatom/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = os.path.join(RESULTS_FOLDER, 'model_SI_LULC.pkl')\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Predict the test labels\n",
    "predicted_labels_test = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the overall accuracy (OA) and the weighted $F_1$ score and the $F_1$ score, precision, and recall for each class separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = np.unique(labels_test)\n",
    "class_names = [lulc_type.name for lulc_type in LULC]\n",
    "mask = np.in1d(predicted_labels_test, labels_test)\n",
    "predictions = predicted_labels_test[mask]\n",
    "true_labels = labels_test[mask]\n",
    "\n",
    "# Extract and display metrics\n",
    "f1_scores = metrics.f1_score(true_labels, predictions, labels=class_labels, average=None)\n",
    "avg_f1_score = metrics.f1_score(true_labels, predictions, average='weighted')\n",
    "recall = metrics.recall_score(true_labels, predictions, labels=class_labels, average=None)\n",
    "precision = metrics.precision_score(true_labels, predictions, labels=class_labels, average=None) \n",
    "accuracy = metrics.accuracy_score(true_labels, predictions)\n",
    "\n",
    "print('Classification accuracy {:.1f}%'.format(100 * accuracy))\n",
    "print('Classification F1-score {:.1f}%'.format(100 * avg_f1_score))\n",
    "print()\n",
    "print('             Class              =  F1  | Recall | Precision')\n",
    "print('         --------------------------------------------------')\n",
    "for idx, lulctype in enumerate([class_names[idx] for idx in class_labels]):\n",
    "    line_data = (lulctype, f1_scores[idx] * 100, recall[idx] * 100, precision[idx] * 100)\n",
    "    print('         * {0:20s} = {1:2.1f} |  {2:2.1f}  | {3:2.1f}'.format(*line_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the standard and transposed Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_confusion_matrix(confusion_matrix, classes, normalize=False, title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, ylabel='True label', xlabel='Predicted label', filename=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    \n",
    "    if normalize:\n",
    "        normalisation_factor = (confusion_matrix.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps)\n",
    "        confusion_matrix = confusion_matrix.astype('float') / normalisation_factor\n",
    "    \n",
    "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title, fontsize=20)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    threshold = confusion_matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):\n",
    "        plt.text(j, i, format(confusion_matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if confusion_matrix[i, j] > threshold else \"black\",\n",
    "                 fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(ylabel, fontsize=20)\n",
    "    plt.xlabel(xlabel, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "confusion_matrix_gbm = metrics.confusion_matrix(true_labels, predictions)\n",
    "plot_confusion_matrix(confusion_matrix_gbm, \n",
    "                      classes=[name for idx, name in enumerate(class_names) if idx in class_labels], \n",
    "                      normalize=True, \n",
    "                      ylabel='Truth (LAND COVER)', \n",
    "                      xlabel='Predicted (GBM)',\n",
    "                      title='Confusion matrix');\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "confusion_matrix_gbm = metrics.confusion_matrix(predictions, true_labels)\n",
    "plot_confusion_matrix(confusion_matrix_gbm, \n",
    "                      classes=[name for idx, name in enumerate(class_names) if idx in class_labels], \n",
    "                      normalize=True, \n",
    "                      xlabel='Truth (LAND COVER)', \n",
    "                      ylabel='Predicted (GBM)',\n",
    "                      title='Transposed Confusion matrix');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the classes the model seems to perform well. Otherwise the training sample is probably too small to make a fair assesment. \n",
    "Additional problems arise due to the unbalanced training set. The image below shows the frequency of the classes used for model training, and we see that the problematic cases are all the under-represented classes: shrubland, water, wetland, and bareland. \n",
    "\n",
    "Improving the reference map would also affect the end result, as, for example some classes are mixed up to some level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "label_ids, label_counts = np.unique(labels_train, return_counts=True)\n",
    "\n",
    "plt.bar(range(len(label_ids)), label_counts)\n",
    "plt.xticks(range(len(label_ids)), [class_names[i] for i in label_ids], rotation=45, fontsize=20);\n",
    "plt.yticks(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves and AUC metrics\n",
    "\n",
    "Calculate precision and recall rates, draw ROC curves and calculate AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = np.unique(np.hstack([labels_test, labels_train]))\n",
    "\n",
    "scores_test = model.predict_proba(features_test)\n",
    "labels_binarized = preprocessing.label_binarize(labels_test, classes=class_labels)\n",
    "\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "for idx,lbl in enumerate(class_labels):\n",
    "    fpr[idx], tpr[idx], _ = metrics.roc_curve(labels_binarized[:, idx], scores_test[:, idx])\n",
    "    roc_auc[idx] = metrics.auc(fpr[idx], tpr[idx])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for idx, lbl in enumerate(class_labels):\n",
    "    if np.isnan(roc_auc[idx]):\n",
    "        continue\n",
    "    plt.plot(fpr[idx], tpr[idx], color=lulc_cmap.colors[lbl],\n",
    "         lw=2, label=class_names[lbl] + ' ({:0.5f})'.format(roc_auc[idx]))\n",
    "    \n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 0.99])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title('ROC Curve', fontsize=20)\n",
    "plt.legend(loc=\"center right\", prop={'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important features\n",
    "\n",
    "Let us now check which features are most important in the above classification. The LightGBM model already contains the information about feature importances, so we only need to query them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names\n",
    "fnames = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12', 'NDVI', 'NDWI', 'NDBI']\n",
    "\n",
    "# Get feature importances and reshape them to dates and features\n",
    "feature_importances = model.feature_importances_.reshape((t, f))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Plot the importances\n",
    "im = ax.imshow(feature_importances, aspect=0.25)\n",
    "plt.xticks(range(len(fnames)), fnames, rotation=45, fontsize=20)\n",
    "plt.yticks(range(t), [f'T{i}' for i in range(t)], fontsize=20)\n",
    "plt.xlabel('Bands and band related features', fontsize=20)\n",
    "plt.ylabel('Time frames', fontsize=20)\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top') \n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=[ax], orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the RGB for the most important date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the RGB image\n",
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n",
    "\n",
    "time_id = np.where(feature_importances == np.max(feature_importances))[0][0]\n",
    "\n",
    "for i in tqdm(range(len(patchIDs))):\n",
    "    sample_path = os.path.join(EOPATCH_SAMPLES_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(sample_path, lazy_loading=True)\n",
    "    ax = axs[i//5][i%5]\n",
    "    ax.imshow(np.clip(eopatch.data['FEATURES'][time_id][..., [2, 1, 0]] * 2.5, 0, 1))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of the results\n",
    "\n",
    "The model has been validated, the remaining thing is to make the prediction on the whole AOI.\n",
    "\n",
    "Here we define a workflow to make the model prediction on the existing EOPatces. The EOTask accepts the features and the names for the labels and scores. The latter is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EOTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictPatchTask(EOTask):\n",
    "    \"\"\"\n",
    "    Task to make model predictions on a patch. Provide the model and the feature, \n",
    "    and the output names of labels and scores (optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, features_feature, predicted_labels_name, predicted_scores_name=None):\n",
    "        self.model = model\n",
    "        self.features_feature = features_feature\n",
    "        self.predicted_labels_name = predicted_labels_name\n",
    "        self.predicted_scores_name = predicted_scores_name\n",
    "        \n",
    "    def execute(self, eopatch):\n",
    "        features = eopatch[self.features_feature]\n",
    "        \n",
    "        t, w, h, f = features.shape\n",
    "        features = np.moveaxis(features, 0, 2).reshape(w * h, t * f)\n",
    "        \n",
    "        predicted_labels = self.model.predict(features)\n",
    "        predicted_labels = predicted_labels.reshape(w, h)\n",
    "        predicted_labels = predicted_labels[..., np.newaxis]\n",
    "        eopatch[(FeatureType.MASK_TIMELESS, self.predicted_labels_name)] = predicted_labels\n",
    "        \n",
    "        if self.predicted_scores_name:\n",
    "            predicted_scores = self.model.predict_proba(features)\n",
    "            _, d = predicted_scores.shape\n",
    "            predicted_scores = predicted_scores.reshape(w, h, d)\n",
    "            eopatch[(FeatureType.DATA_TIMELESS, self.predicted_scores_name)] = predicted_scores\n",
    "        \n",
    "        return eopatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tasks and the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EXISTING EOPATCHES\n",
    "load = LoadTask(EOPATCH_SAMPLES_FOLDER)\n",
    "\n",
    "# PREDICT\n",
    "predict = PredictPatchTask(model, (FeatureType.DATA, 'FEATURES'), 'LBL_GBM', 'SCR_GBM')\n",
    "\n",
    "# SAVE\n",
    "save = SaveTask(EOPATCH_SAMPLES_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "# EXPORT TIFF\n",
    "export_tiff = ExportToTiffTask((FeatureType.MASK_TIMELESS, 'LBL_GBM'))\n",
    "tiff_location = os.path.join(RESULTS_FOLDER, 'predicted_tiff')\n",
    "os.makedirs(tiff_location, exist_ok=True)\n",
    "\n",
    "workflow = LinearWorkflow(\n",
    "    load,\n",
    "    predict,\n",
    "    export_tiff,\n",
    "    save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the prediction and export to GeoTIFF images\n",
    "\n",
    "Here we use the `EOExecutor` to run the workflow in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of execution arguments for each patch\n",
    "execution_args = []\n",
    "for i in range(len(patchIDs)):\n",
    "    execution_args.append(\n",
    "        {\n",
    "            load: {'eopatch_folder': f'eopatch_{i}'},\n",
    "            export_tiff: {'filename': f'{tiff_location}/prediction_eopatch_{i}.tiff'},\n",
    "            save: {'eopatch_folder': f'eopatch_{i}'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Run the executor\n",
    "executor = EOExecutor(workflow, execution_args)\n",
    "executor.run(workers=1, multiprocess=False)\n",
    "executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(f'Execution failed EOPatches with IDs:\\n{failed_ids}\\n'\n",
    "                       f'For more info check report at {executor.get_report_filename()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Merge tiffs with gdal_merge.py (with compression) using bash command magic\n",
    "# gdal has to be installed on your computer!\n",
    "!gdal_merge.py -o results/predicted_tiff/merged_prediction.tiff -co compress=LZW results/predicted_tiff/prediction_eopatch_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(20, 25))\n",
    "\n",
    "for i in tqdm(range(len(patchIDs))):\n",
    "    eopatch_path = os.path.join(EOPATCH_SAMPLES_FOLDER, f'eopatch_{i}')\n",
    "    eopatch = EOPatch.load(eopatch_path, lazy_loading=True)\n",
    "    ax = axs[i//5][i%5]\n",
    "    im = ax.imshow(eopatch.mask_timeless['LBL_GBM'].squeeze(), cmap=lulc_cmap, norm=lulc_norm)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(\"auto\")\n",
    "    del eopatch\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "cb = fig.colorbar(im, ax=axs.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20) \n",
    "cb.set_ticks([entry.id for entry in LULC])\n",
    "cb.ax.set_xticklabels([entry.name for entry in LULC], rotation=45, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual inspection of patches\n",
    "\n",
    "Here is just a simple piece of code that allows a closer inspection of the predicted labels. \n",
    "\n",
    "Random subsets of patches are chosen, where prediction and ground truth are compared. For visual aid the mask of differences and the true color image are also provided.\n",
    "\n",
    "In majority of the cases, differences seem to lie on the border of different structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the Reference map\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "idx = np.random.choice(range(len(patchIDs)))\n",
    "inspect_size = 100\n",
    "\n",
    "eopatch = EOPatch.load(os.path.join(EOPATCH_SAMPLES_FOLDER, f'eopatch_{idx}'), lazy_loading=True)\n",
    "\n",
    "w, h = eopatch.mask_timeless['LULC'].squeeze().shape\n",
    "\n",
    "w_min = np.random.choice(range(w - inspect_size))\n",
    "w_max = w_min + inspect_size\n",
    "h_min = np.random.choice(range(h - inspect_size))\n",
    "h_max = h_min + inspect_size\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "plt.imshow(eopatch.mask_timeless['LULC'].squeeze()[w_min:w_max, h_min:h_max], cmap=lulc_cmap, norm=lulc_norm)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('Ground Truth', fontsize=20)\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "plt.imshow(\n",
    "    eopatch.mask_timeless['LBL_GBM'].squeeze()[w_min:w_max, h_min:h_max], cmap=lulc_cmap, norm=lulc_norm)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('Prediction', fontsize=20)\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "mask = eopatch.mask_timeless['LBL_GBM'].squeeze() != eopatch.mask_timeless['LULC'].squeeze()\n",
    "plt.imshow(mask[w_min:w_max, h_min:h_max], cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('Difference', fontsize=20)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "image = np.clip(eopatch.data['FEATURES'][8][..., [2, 1, 0]] * 3.5, 0, 1)\n",
    "plt.imshow(image[w_min:w_max, h_min:h_max])\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n",
    "ax.set_aspect(\"auto\")\n",
    "plt.title('True Color', fontsize=20)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
